# Audio学习总结（A）

# 基础知识

handfree 免提

speaker



## PCM

### PCM数据

对声音进行PCM编码生成的是二进制序列(如下图所示), 姑且称之为[PCM数据](https://en.wikipedia.org/wiki/Raw_audio_format), 不包含附加信息, 经PCM编码的数据通常认为是无损编码
![PCM_DATA_FORMAT](https://images2017.cnblogs.com/blog/417887/201801/417887-20180108234320676-1670909448.jpg)

PCM数据包含大小换算方式为: **数据大小 = 采样率 \* 采样位数 \* 声道数 \* 播放时间=比特率×播放时间**

```shell
#一般在分析音频卡顿问题时，可以通过dump pcm数据来定位哪个过程中发生卡顿，dump方法如下：
adb root
adb shell
echo "vbc_pcm_dump=vbc_dac0" > /data/vendor/local/media/mmi.audio.ctrl //
echo set_dump_data_switch=0x0fff > /data/vendor/local/media/mmi.audio.ctrl
echo set_dump_data_path=/data/vendor/local/media/ > /data/vendor/local/media/mmi.audio.ctrl
#在复现问题前输入上面命令，然后复现问题，复现后将/data/vendor/local/media/这个路径下的dump开头pull出来，用audacity来分析pcm文件在哪个过程出现问题。

#VBC是Voice Band Controler的缩写。是一个音频数字处理模块（即音频驱动中的DAI），带IIS接口/TDM接口，IIS接口符合是标准的IIS协议规范，对音频数据流做播放、采集及算法处理。详见《VBC模块及验证介绍.docx》
```

### 音频参数

通过Sample Rate,Sample Size,Number of Channels数据描述一个PCM数据。

**采样频率**（sample rate or frame rate），也称采样率或帧率 是指录音设备在单位时间内对声音信号的采样数或样本数, 单位为Hz(赫兹), 采样频率越高能表现的频率范围就越大

> 一些常用音频采样率如下:
> 8kHz    - 电话所用采样率
> 22.05kHz - 无线电广播所用采样率
> 44.1kHz  - 音频CD, 也常用于 MPEG-1 音频(VCD, SVCD, MP3)所用采样率
> 48kHz   - miniDV、数字电视、DVD、DAT、电影和专业音频所用的数字声音所用采样率

**采样位数**(Bit Depth, Sample Format, Sample Size, Sample Width), 也称**位深度**, 是指采集卡在采集和播放声音文件时所使用数字声音信号的二进制位数, 或者说是每个采样样本所包含的位数, 通常有8 bit、16 bit

**声道数**(Channel), 是指采集卡在采集时使用声道数, 分为单声道(Mono)和双声道/立体声(Stereo)

**比特率**(Bit Rate), 也称位率, 指每秒传送的比特(bit)数, 单位为bps(Bit Per Second), 比特率越高, 传送数据速度越快. 声音中的比特率是指将模拟声音信号转换成数字声音信号后, 单位时间内的二进制数据量
其计算公式为: **比特率 = 采样频率 \* 采样位数 \* 声道数**

**音频帧**（Frame）,PCM不需要音频帧概念，大小固定，ARM规定的音频帧每20ms的音频是1帧，MP3的比较复杂

## 音频编码和音频文件

### 音频编码格式

通常会对音频数据按照一定格式进行编码压缩, 从而缩小其占用空间，常见音频编码格式有

**MP3**: MPEG-1/2 Audio Layer III, 利用人耳对高频声音信号不敏感的特性对数据进行有损压缩, 同时保证信号不失真, 采用该编码格式的压缩率可达1:10甚至1:12, 是目前最常用的一种音频编码技术.

**WMA**: Windows Media Audio, 是微软推出的一种音频编码, 以减少数据流量但保持音质的方法来达到更高的压缩率, 其压缩率一般可以达到1:18, 是一种有损压缩. 不过最新版本推出了无损压缩方式.

**Vorbis**: 开源音频编码, 没有专利限制, 能够在相对低的比特率下实现比MP3更好的音质(?)

**SBC**: 传输蓝牙音频(A2DP)时的一种默认编码解码格式

**G.711**: ITU-T制定的音频编码方式, 主要用于电话和VoIP

**AAC**: Advanced Audio Coding, 是MP3的继任者, 在相同的比特率上实现比MP3更好的音质, 也是一种有损压缩

**FLAC**: Free Lossless Audio Codec, 是一种无损音频压缩编码, 也是目前的主流无损编码格式.

**APE**: Monkey's Audio, 是一种无损压缩编码格式, 压缩率可达1:2

**AMR**：Adaptive Multi-Rate，自适应多速率，是一种音频编码文件格式，专用于有效地压缩语音频率。 AMR音频主要用于移动设备的音频压缩，压缩比非常高，但是音质比较差，主要用于语音类的音频压缩，不适合对音质要求较高的音乐类音频的压缩。
**Ogg**：OGGVobis(oggVorbis)是一种[音频](https://baike.baidu.com/item/音频)[压缩格式](https://baike.baidu.com/item/压缩格式)，类似于[MP3](https://baike.baidu.com/item/MP3/23904)等的[音乐格式](https://baike.baidu.com/item/音乐格式/3043629)。Ogg是完全免费、开放和没有专利限制的。OggVorbis文件的扩展名是".ogg"。Ogg文件格式可以不断地进行大小和音质的改良，而不影响旧有的[编码器](https://baike.baidu.com/item/编码器/6029803)或播放器。
**PCM**：pcm是没有压缩的编码方式

### 音频文件格式

音频编码是为了压缩其大小, 在计算机等设备上保存为文件时有不同的格式, 即[音频文件格式](https://en.wikipedia.org/wiki/Audio_file_format)
文件格式类似于一个容器, 有固定的结构, 通常至少包含下面两部分
\- 头部:    音频文件的元数据, 如采样率、声道模式等参数
\- 数据部分: 经过编码后的音频数据流

不同音频编码格式都有其对应的文件格式, 比如
MP3编码音频文件封装在MP3文件格式中, 以.mp3为后缀, 同理WAV、FLAC、AAC等
Vorbis则封装在Ogg文件格式中, 以.ogg为后缀

[WAVE](http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html)格式音频(扩展名为".wav")是常见的一种音频文件格式, 采用RIFF文件格式结构
通常用来封装PCM数据, 可认为是无损保存, 也是主流系统不需要解码器就可以读取的格式.

> \- 音频文件中通常还[ID3](http://id3.org/)信息, 它保存了歌手、标题、专辑名称、年代、风格等信息
> \- 视频文件格式也可以存储音频数据流, 如MP4、Matroska

### unisoc支持的媒体音频格式如下

音频格式
.3gp .mp4 .m4a .m4r .m4b .mov .mka .aac .mp3 .amr .3g2 .awb .flac .wav .ogg .midi
音频解码格式
aac amr mp3 flac vorbis midi
音频编码格式
aac amr mp3

### 音频编解码库和工具

音频数据编码可以对其大小进行压缩, 但是编码和解码过程是需要计算机使用编解码器([codec](https://en.wikipedia.org/wiki/List_of_codecs))来完成.
各音频编码通常都有自己的编解码库和工具, 不过实际在使用过程中用户期望的是某个库和工具能包罗万象.
于是就有了[FFmpeg](https://ffmpeg.org/)、[Libav](https://libav.org/)、[Xvid](https://www.xvid.com/)等知名工具, 他们可以完成各种格式的音视频编解码.

## 多媒体播放组件介绍

MediaPlayer

Nuplayer

MediaCodec

Acodec

OXM

Stagefright



------



# 音频架构

## Android Audio系统的框架

**参考：**[**https://source.android.google.cn/devices/audio**](https://source.android.google.cn/devices/audio)

| 分层         | Audio管理环节             | Audio输出                | Audio输入                   |
| ------------ | ------------------------- | ------------------------ | --------------------------- |
| Java层       | android.media.AudioSystem | android.media.AudioTrack | android.media.AudioRecorder |
| 本地框架层   | AudioSystem               | AudioTrack               | AudioRecorder               |
| AudioFlinger | IAudioFlinger             | IAudioTrack              | IAudioRecorder              |
| 硬件抽象层   | AudioHardwareInterface    | AudioStreamOut           | AudioStreamIn               |

**一、Audio Framework模块结构图**

![image-20200831151731171](C:\Users\hursion.zhang\AppData\Roaming\Typora\typora-user-images\image-20200831151731171.png)

 **1、应用框架**

​    应用框架包含应用代码，该代码可使用 android.media API 与音频硬件进行互动。在内部，此代码会调用相应的 JNI 粘合类，以访问与音频硬件互动的原生代码。代码位于frameworks\base\media\

​    Extractors：做数据分离，将audio数据分解成以frame分单位，送给decoder解码。

​    Codec：用于video | audio数据的解码。

​    AudioRecord：提供给应用，用于录音的接口。

​    AudioTrack：提供给应用，用于播放audio的接口。

​    MediaPlayer：提供给应用，用于播放音视频的接口。和AudioTrack的区别在于：AudioTrack只能播放PCM流音频。MediaPlayer关于音频方面的具体实现还是AudioTrack。

​    AudioService：系统服务，提供应用所需的除了播放的相关的audio业务，与之对应的Client端是AudioManager，AudioManager运行在应用进程。两者之间通过binder进行通信。

​    AudioManager、AudioService及AudioSystem等类提供声音控制、通道选择、音效设置等功能。

​    TIF：TvInputFramework的缩写，提供应用所需的TV相关业务。

**2、JNI**

​    与 android.media 关联的 JNI 代码可调用较低级别的原生代码，以访问音频硬件。JNI 位于 frameworks/base/core/jni/ 和 frameworks/base/media/jni 中。

**3、native框架**

​    native框架可提供相当于 android.media 软件包的原生软件包，从而调用 Binder IPC 代理以访问媒体服务器的特定于音频的服务。原生框架代码位于 frameworks/av/media/libmedia 中。

**4、Binder IPC**

​    Binder IPC 代理用于促进跨越进程边界的通信。代理位于 frameworks/av/media/libmedia 中，并以字母“I”开头。

**5、媒体服务器**

​    媒体服务器包含音频服务，这些音频服务是与您的 HAL 实现进行互动的实际代码。媒体服务器位于 frameworks/av/services/中。

Client:

​    MediaCodec/AudioRecord/AudioTrack/MediaPlayer/AudioSystem， 与Frameworks Java层的相关类一一对应。

Service:

​    AudioFlinger和AudioPolicyService是核心系统服务，运行在AudioServer系统进程。

​    AudioPolicyService：Audio策略的制定者。规划各类音频流的输出路径，指定硬件设备（听筒喇叭，耳机，蓝牙等）。同时它是AudioFlinger的client端，会调用到AudioFlinger。

​    AudioFlinger：Audio策略的执行者。向上为上层提供访问和管理音频的接口，向下通过Hal来管理Audio硬件设备，具体涉及怎么和设备通信，如何处理等。

**6、HAL**

​    HAL 定义了由音频服务调用且您必须实现以确保音频硬件功能正常运行的标准接口。音频 HAL 接口位于 hardware/libhardware/include/hardware 中。参阅 audio.h。

**7、内核驱动程序。**

​    音频驱动程序可与您的硬件和 HAL 实现进行互动。您可以使用高级 Linux 声音体系 (ALSA)、开放声音系统 (OSS) 或自定义驱动程序（HAL 与驱动程序无关）。

注意：如果您使用 ALSA，我们建议将 external/tinyalsa 用于驱动程序的用户部分，因为它具有兼容的许可（标准的用户模式库已获得 GPL 许可）。

![](https://img-blog.csdnimg.cn/20190509093348529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eHk5MTA4MTA=,size_16,color_FFFFFF,t_70)

## 多媒体架构变更

![mediaserver 强化](https://source.android.google.cn/devices/media/images/ape_media_split.png?hl=zh-cn)

旧版 Android 使用单个整体的 `mediaserver` 进程，该进程具有众多权限（相机访问权、音频访问权、视频驱动程序访问权、文件访问权、网络访问权等）。Android 7.0 将 `mediaserver` 进程拆分为几个新进程，这些进程要求的权限要少得多：

**图 1.** mediaserver 强化的架构变更

采用这种新型架构之后，即使某个进程遭到入侵，恶意代码也无法获得之前 mediaserver 所拥有的全部权限。进程受 SElinux 和 seccomp 政策的限制。

**HIDL 的概念**

　　HIDL 读作 hide-l，全称是 Hardware Interface Definition Language。目的是使 Android 可以在不重新编译 HAL 的情况下对 Framework 进行 OTA 升级。 
　　使用 HIDL 描述的 HAL 描述文件替换旧的用头文件描述的 HAL 文件的过程称为 **HAL 的 binder 化**（binderization）。
　　已发布的 HIDL package包位于 Android 代码库的`hardware/interfaces/`或`vendor/<vendorName>`目录下。使用 HDIL 描述的 HAL 接口存放在这些目录下的`.hal`文件中。比如我们可以在`hardware/interfaces/audio/`目录下找到部分 Audio HAL 描述文件，另外在frameworks/av/media/下多了个文件libaudiohal 

## 核心类介绍

![Android_Audio_Architecture](https://img-blog.csdn.net/20170301225246685?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl1YW55dW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```java
PlayerBase.java
AudioSystem
AudioFlinger：Audio系统的工作引擎，管理系统的输入输出音频流，并承担音频数据的混音，以及读写audio硬件等工作以实现数据的输入输出功能
IAudioFlinger
AudioHardwareInterface

AudioTrack：对外的API，提供音频数据的输出；
IAudioTrack
AudioStreamOut

AudioRecorder：对外的API，提供音频数据的采集；
IAudioRecorder
AudioStreamIn

AudioPolicyService：Audio系统的策略控制中心，具体掌控系统声音设备的选择和切换，音量控制等功能；

AudioAttributes：，基于流类型混音策略过于受限，音频属性为音频系统提供有关特定音频源的详细信息，包括用法（播放来源的原因）、内容类型（播放来源的类型）、标记（来源的播放方式）以及上下文（Android 9 中的新变化），音频系统会使用这些属性做出混音决策并将系统状态通知给应用。
    
frameworks/av/services/audiopolicy/enginedefault/src/Engine.cpp
frameworks/av/services/audiopolicy/managerdefault/AudioPolicyManager.cpp
 
    
//so文件
sl8541e_1h10_32b:/system # find ./ -name libmedia*                                                                                                                                                         
./lib/libmedia.so
./lib/libmedia2_jni_core.so
./lib/libmedia_helper.so
./lib/libmedia_jni.so
./lib/libmedia_jni_utils.so
./lib/libmedia_omx.so
./lib/libmedia_omx_client.so
./lib/libmediadrm.so
./lib/libmediadrmmetrics_lite.so
./lib/libmediaextractorservice.so
./lib/libmedialogservice.so
./lib/libmediametrics.so
./lib/libmediandk.so
./lib/libmediandk_utils.so
./lib/libmediaplayerservice.so
./lib/libmediautils.so
./lib/vndk-29/libmedia_helper.so
./lib/vndk-29/libmedia_omx.so
sl8541e_1h10_32b:/system # find ./ -name libaudio*                                                                                                                                                         
./lib/libaudioclient.so
./lib/libaudioeffect_jni.so
./lib/libaudioflinger.so
./lib/libaudiohal.so
./lib/libaudiohal@2.0.so
./lib/libaudiohal@4.0.so
./lib/libaudiohal@5.0.so
./lib/libaudiohal_deathhandler.so
./lib/libaudiomanager.so
./lib/libaudiopolicy.so
./lib/libaudiopolicyengineconfigurable.so
./lib/libaudiopolicymanager.so
./lib/libaudiopolicymanagerdefault.so
./lib/libaudiopolicyservice.so
./lib/libaudioprocessing.so
./lib/libaudiospdif.so
./lib/libaudioutils.so
./lib/vndk-29/libaudioroute.so
./lib/vndk-29/libaudioutils.so

```

# MediaPlayer到ACodec到OMX流程

例子：

```java
MediaPlayer基本使用方式：播放一首MP3歌曲
MediaPlayer mp = new MediaPlayer();
//1.方法来设置音频文件的路径 
mp.setDataSource("/sdcard/test.mp3");
//2.MediaPlayer进入到准备状态 
mp.prepare();
//3.开始播放音频 
mp.start();
```

![img](https://img-blog.csdn.net/20180901181051695?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAxNjQxOTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 1.setDataSource流程

时序图参考如下：

![MediaPlayer_setDataSource](V:\codedir\10.0_automotive\0_UML\MediaPlayer_setDataSource.png)

```java
//上层设置setDataSource不同参数，最终都要调用private方法设置参数,
///sprdroid10_trunk_19c/frameworks/base/media/java/android/media/MediaPlayer.java
1166      private void setDataSource(String path, String[] keys, String[] values,
1167              List<HttpCookie> cookies)
1168              throws IOException, IllegalArgumentException, SecurityException, IllegalStateException {
...
1175              nativeSetDataSource(
1176                  MediaHTTPService.createHttpServiceBinderIfNecessary(path, cookies),
1177                  path,
1178                  keys,
1179                  values);
//定义nativeSetDataSource，实现在jni中   
1189      private native void nativeSetDataSource(
1190          IBinder httpServiceBinder, String path, String[] keys, String[] values)
1191          throws IOException, IllegalArgumentException, SecurityException, IllegalStateException;

```

nativeSetDataSource提供jni调用android_media_MediaPlayer_setDataSourceAndHeaders

```c++
///sprdroid10_trunk_19c/frameworks/base/media/jni/android_media_MediaPlayer.cpp
 android_media_MediaPlayer_setDataSourceAndHeaders(
222          JNIEnv *env, jobject thiz, jobject httpServiceBinderObj, jstring path,
223          jobjectArray keys, jobjectArray values) {
224  
225      sp<MediaPlayer> mp = getMediaPlayer(env, thiz);
...
    //调用native 的MediaPlayer setDataSource
259      status_t opStatus =
260          mp->setDataSource(
261                  httpService,
262                  pathStr,
263                  headersVector.size() > 0? &headersVector : NULL);

   
167  static sp<MediaPlayer> getMediaPlayer(JNIEnv* env, jobject thiz)
168  {
170      MediaPlayer* const p = (MediaPlayer*)env->GetLongField(thiz, fields.context);
172  }
```

mediaplayer::setDataSource

```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmedia/mediaplayer.cpp
155  status_t MediaPlayer::setDataSource(...)
164              sp<IMediaPlayer> player(service->create(this, mAudioSessionId));
    //调用IMediaPlayer：：setDataSource
165              if ((NO_ERROR != doSetRetransmitEndpoint(player)) ||
166                  (NO_ERROR != player->setDataSource(httpService, url, headers))) {}


```

IMediaPlayer是个抽象基类

```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmedia/include/media/IMediaPlayer.h
47  class IMediaPlayer: public IInterface
48  {
    //声明了setDataSource纯虚函数
59      virtual status_t        setDataSource(int fd, int64_t offset, int64_t length) = 0;
60      virtual status_t        setDataSource(const sp<IStreamSource>& source) = 0;
61      virtual status_t        setDataSource(const sp<IDataSource>& source) = 0;
    //申明服务端BnMediaPlayer
143  class BnMediaPlayer: public BnInterface<IMediaPlayer>
144  {
145  public:
146      virtual status_t    onTransact( uint32_t code,
147                                      const Parcel& data,
148                                      Parcel* reply,
149                                      uint32_t flags = 0);
150  };

```

 BpMediaPlayer继承了IMediaPlayer并实现了其抽象方法

```cpp
 ///sprdroid10_trunk_19c/frameworks/av/media/libmedia/IMediaPlayer.cpp   
99  class BpMediaPlayer: public BpInterface<IMediaPlayer>
100  {
	//客户端实现setDataSource
115      status_t setDataSource(){
    //发送SET_DATA_SOURCE_STREAM命令在Bn端远程调用setDataSource()方法
137          remote()->transact(SET_DATA_SOURCE_STREAM, data, &reply);
     //客户端通过reply.readInt32()方法返回Bn远程端返回的数据，返回给java应用层使用
138          return reply.readInt32();
139      }
    //通过binder调用本地端onTransact

623  IMPLEMENT_META_INTERFACE(MediaPlayer, "android.media.IMediaPlayer");

627  status_t BnMediaPlayer::onTransact(
628      uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)
629  {
630      switch (code) {
...
669          case SET_DATA_SOURCE_STREAM: {
670              CHECK_INTERFACE(IMediaPlayer, data, reply);
671              sp<IStreamSource> source =
672                  interface_cast<IStreamSource>(data.readStrongBinder());
673              if (source == NULL) {
674                  reply->writeInt32(BAD_VALUE);
675              } else {
    //调用BnMediaPlayer的setDataSource
676                  reply->writeInt32(setDataSource(source));
677              }
678              return NO_ERROR;
679          }
```

服务端BnMediaPlayer的实现

```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmediaplayerservice/MediaPlayerService.h
//
	class Client : public BnMediaPlayer {
314          // IMediaPlayer interface


////sprdroid10_trunk_19c/frameworks/av/media/libmediaplayerservice/MediaPlayerService.cpp
874  status_t MediaPlayerService::Client::setDataSource(
875          const sp<IStreamSource> &source) {
876      // create the right type of player
877      player_type playerType = MediaPlayerFactory::getPlayerType(this, source);
         //p的实例
878      sp<MediaPlayerBase> p = setDataSource_pre(playerType);
879      if (p == NULL) {
880          return NO_INIT;
881      }
882  
883      // now set data source
    //交给p->setDataSource设置状态，最终调用NuPlayerDriver.cpp中的status_t NuPlayerDriver::setDataSource()
884      return mStatus = setDataSource_post(p, p->setDataSource(source));
885  }
        
667  sp<MediaPlayerBase> MediaPlayerService::Client::setDataSource_pre(
668          player_type playerType)
669  {
670      ALOGV("player type = %d", playerType);
671  
672      // create the right type of player
673      sp<MediaPlayerBase> p = createPlayer(playerType);
674      if (p == NULL) {
675          return p;
676      }
697      });
        
637  sp<MediaPlayerBase> MediaPlayerService::Client::createPlayer(player_type playerType)
638  {
639      // determine if we have the right player type
640      sp<MediaPlayerBase> p = getPlayer();
641      if ((p != NULL) && (p->playerType() != playerType)) {
642          ALOGV("delete player");
643          p.clear();
644      }
645      if (p == NULL) {
    //由MediaPlayerFactory来创建player
646          p = MediaPlayerFactory::createPlayer(playerType, mListener, mPid);
647      }
654  }
```

MediaPlayerFactory::createPlayer

```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmediaplayerservice/MediaPlayerFactory.cpp
127  sp<MediaPlayerBase> MediaPlayerFactory::createPlayer(
128          player_type playerType,
129          const sp<MediaPlayerBase::Listener> &listener,
130          pid_t pid) {
131      sp<MediaPlayerBase> p;
132      IFactory* factory;
133      status_t init_result;
134      Mutex::Autolock lock_(&sLock);

141  //playerType通过MediaPlayerFactory::getPlayerType确定使用Nuplayer还是定制的Player。然后创建对应Player
142      factory = sFactoryMap.valueFor(playerType);
143      CHECK(NULL != factory);
144      p = factory->createPlayer(pid);

162  }

//NuPlayerFactory继承MediaPlayerFactory内部类IFactory
170  class NuPlayerFactory : public MediaPlayerFactory::IFactory {
171    public:
172      virtual float scoreFactory(const sp<IMediaPlayer>& /*client*/,
173                                 const char* url,
174                                 float curScore) {
202      }
    //创建NuPlayerDriver对象
217      virtual sp<MediaPlayerBase> createPlayer(pid_t pid) {
218          ALOGV(" create NuPlayer");
219          return new NuPlayerDriver(pid);
220      }
221  };
```

NuPlayerDriver::setDataSource

```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmediaplayerservice/nuplayer/NuPlayerDriver.cpp
184  status_t NuPlayerDriver::setDataSource(const sp<IStreamSource> &source) {
185      ALOGV("setDataSource(%p) stream source", this);
186      Mutex::Autolock autoLock(mLock);
187  
188      if (mState != STATE_IDLE) {
189          return INVALID_OPERATION;
190      }
191  
192      mState = STATE_SET_DATASOURCE_PENDING;
193  //交给Nuplayer
194      mPlayer->setDataSourceAsync(source);
195  
196      while (mState == STATE_SET_DATASOURCE_PENDING) {
197          mCondition.wait(mLock);
198      }
199  
200      return mAsyncResult;
201  }
```



```c++
///sprdroid10_trunk_19c/frameworks/av/media/libmediaplayerservice/nuplayer/NuPlayer.cpp
231  void NuPlayer::setDataSourceAsync(const sp<IStreamSource> &source) {
    //发送命令kWhatSetDataSource和返回命令kWhatSourceNotify消息命令
232      sp<AMessage> msg = new AMessage(kWhatSetDataSource, this);
233  
234      sp<AMessage> notify = new AMessage(kWhatSourceNotify, this);
235  
236      msg->setObject("source", new StreamingSource(notify, source));
237      msg->post();
238      mDataSourceType = DATA_SOURCE_TYPE_STREAM;
239  }

549  void NuPlayer::onMessageReceived(const sp<AMessage> &msg) {
550      switch (msg->what()) {
551          case kWhatSetDataSource:
552          {
553              ALOGV("kWhatSetDataSource");
554  
555              CHECK(mSource == NULL);
556  
557              status_t err = OK;
558              sp<RefBase> obj;
559              CHECK(msg->findObject("source", &obj));
560              if (obj != NULL) {
561                  Mutex::Autolock autoLock(mSourceLock);
562                  mSource = static_cast<Source *>(obj.get());
563              } else {
564                  err = UNKNOWN_ERROR;
565              }
566  
567              CHECK(mDriver != NULL);
568              sp<NuPlayerDriver> driver = mDriver.promote();
569              if (driver != NULL) {
    //Completed
570                  driver->notifySetDataSourceCompleted(err);
571              }
572              break;
573          }
```

# AudioServer初始化流程

## init启动AudioServer

![](https://img-blog.csdnimg.cn/2019033112023460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmJpeGluZzEyMw==,size_16,color_FFFFFF,t_70)

AudioFlinger和AudioPolicyService是音频的系统核心服务，运行在AudioServer中，由init启动。

```shell
debug技巧
1.adb shell dumpsys media.audio_flinger >audioflinger.txt
2.adb shell dumpsys media.audio_policy >audiopolicy.txt
3.adb shell dumpsys audio>audio.txt
```



```c
#frameworks\av\media\audioserver\audioserver.rc
service audioserver /system/bin/audioserver
    class core
    user audioserver
    # media gid needed for /dev/fm (radio) and for /data/misc/media (tee)
    group audio camera drmrpc inet media mediadrm net_bt net_bt_admin net_bw_acct wakelock
    capabilities BLOCK_SUSPEND
    ioprio rt 4
    writepid /dev/cpuset/foreground/tasks /dev/stune/foreground/tasks
    #重启audioserver时，需要重启vendor.audio-hal-2-0、vendor.audio-hal-4-0-msd以及audio-hal-2-0
    onrestart restart vendor.audio-hal-2-0 
    onrestart restart vendor.audio-hal-4-0-msd 
    # Keep the original service name for backward compatibility when upgrading
    # O-MR1 devices with framework-only.
    onrestart restart audio-hal-2-0
    
#vts.native_server.on属性可以启动和停止audioserve
on property:vts.native_server.on=1
    stop audioserver
on property:vts.native_server.on=0
    start audioserver

on init
    mkdir /dev/socket/audioserver 0775 audioserver audioserver
```

![image-20200828094851520](C:\Users\hursion.zhang\AppData\Roaming\Typora\typora-user-images\image-20200828094851520.png)



audioserver的入口在`frameworks/av/media/audioserver/main_audioserver.cpp`中

```cpp
//frameworks/av/media/audioserver/main_audioserver.cpp
int main(int argc __unused, char **argv)
{
...
    //初始化media.log 系统，https://source.android.google.cn/devices/audio/debugging#mediaLog
        MediaLogService::instantiate();
    ...
        //初始化AudioFlinger与AudioPolicyService
        AudioFlinger::instantiate();
        AudioPolicyService::instantiate();
...
        if (mmapPolicy == AAUDIO_POLICY_AUTO || mmapPolicy == AAUDIO_POLICY_ALWAYS) {
            //AAudioService是8.0推出的，通过mmap支持的HAL层和驱动结合，能够缩短延迟时间的服务，https://source.android.google.cn/devices/audio/aaudio
            AAudioService::instantiate();
        }
...
    	//SoundTrigger是语音识别服务 
        SoundTriggerHwService::instantiate();
     	//监听Binder是否有服务与AudioServer通信
        ProcessState::self()->startThreadPool();
        IPCThreadState::self()->joinThreadPool();
```

## AudioFlinger::instantiate()

AudioFlinger首先通过instantiate方法在audiosever中进行初始化。其继承了BinderService和BnAudioFlinger，可用于在Servicemanager中对服务AudioFlinger进行注册。

![AudioFlinger](V:\codedir\10.0_automotive\0_UML\AudioFlinger.png)

[frameworks](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/)/[av](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/)/[services](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/)/[audioflinger](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/audioflinger/)/[AudioFlinger.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/audioflinger/AudioFlinger.h)

```c++
116  class AudioFlinger :
117      public BinderService<AudioFlinger>,
118      public BnAudioFlinger
119  {
120      friend class BinderService<AudioFlinger>;   // for AudioFlinger()
```

[frameworks](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/)/[native](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/)/[libs](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/libs/)/[binder](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/libs/binder/)/[include](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/libs/binder/include/)/[binder](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/libs/binder/include/binder/)/[BinderService.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/native/libs/binder/include/binder/BinderService.h)

```c++
34  class BinderService
35  {
36  public:
37      static status_t publish(bool allowIsolated = false,
38                              int dumpFlags = IServiceManager::DUMP_FLAG_PRIORITY_DEFAULT) {
39          sp<IServiceManager> sm(defaultServiceManager());
    		//新建服务SERVICE，并将服务加到ServiceManager中。
40          return sm->addService(String16(SERVICE::getServiceName()), new SERVICE(), allowIsolated,
41                                dumpFlags);
42      }
43  
44      static void publishAndJoinThreadPool(
45              bool allowIsolated = false,
46              int dumpFlags = IServiceManager::DUMP_FLAG_PRIORITY_DEFAULT) {
47          publish(allowIsolated, dumpFlags);
48          joinThreadPool();
49      }
50  
51      static void instantiate() { publish(); }
52  
53      static status_t shutdown() { return NO_ERROR; }
54  
55  private:
56      static void joinThreadPool() {
57          sp<ProcessState> ps(ProcessState::self());
58          ps->startThreadPool();
59          ps->giveThreadPoolName();
60          IPCThreadState::self()->joinThreadPool();
61      }
62  };
```

getServiceName需要在服务(AudioFlinger)中实现，AudioFlinger的服务名为”media.audio_flinger”。也可以在adb中通过service list查询当前系统的服务包括哪些。也可以通过dumpsys media.audio_flinger将AudioFlinger的关键信息打印出来。

[frameworks](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/)/[av](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/)/[services](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/)/[audioflinger](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/audioflinger/)/[AudioFlinger.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/frameworks/av/services/audioflinger/AudioFlinger.h)

```c
123      static const char* getServiceName() ANDROID_API { return "media.audio_flinger"; }
```

当instantiate将服务注册后，可转到服务的实现分析，由于AudioFlinger继承了RefBase类，所以能够被智能指针引用，在publish中的addService时，参数二实质是被sp引用，所以AudioFlinger会在构造方法之后调用onFirstRef方法。

### AudioFlinger::AudioFlinger()

```c++
AudioFlinger::AudioFlinger()
    : BnAudioFlinger(),
      mMediaLogNotifier(new AudioFlinger::MediaLogNotifier()),
      mPrimaryHardwareDev(NULL),
      mAudioHwDevs(NULL),
      mHardwareStatus(AUDIO_HW_IDLE),
      mMasterVolume(1.0f),
      mMasterMute(false),
      // mNextUniqueId(AUDIO_UNIQUE_ID_USE_MAX),
      mMode(AUDIO_MODE_INVALID),
      mBtNrecIsOff(false),
      mIsLowRamDevice(true),
      mIsDeviceTypeKnown(false),
      mTotalMemory(0),
      mClientSharedHeapSize(kMinimumClientSharedHeapSizeBytes),
      mGlobalEffectEnableTime(0),
      mPatchPanel(this),
      mSystemReady(false)
{
    // unsigned instead of audio_unique_id_use_t, because ++ operator is unavailable for enum
    for (unsigned use = AUDIO_UNIQUE_ID_USE_UNSPECIFIED; use < AUDIO_UNIQUE_ID_USE_MAX; use++) {
        // zero ID has a special meaning, so unavailable
        mNextUniqueIds[use] = AUDIO_UNIQUE_ID_USE_MAX;
    }

    /* get property and set mSYSTEM_READY */
    const bool systemreadyStatus = property_get_bool("af.media.systemready.state", false);
    if (systemreadyStatus) {
       mSystemReady = true;
    }
	//通过ro.test_harness来判断是否开启media.log系统
    const bool doLog = property_get_bool("ro.test_harness", false);
    if (doLog) {
        mLogMemoryDealer = new MemoryDealer(kLogMemorySize, "LogWriters",
                MemoryHeapBase::READ_ONLY);
        (void) pthread_once(&sMediaLogOnce, sMediaLogInit);
    }

    // reset battery stats.
    // if the audio service has crashed, battery stats could be left
    // in bad state, reset the state upon service start.
    BatteryNotifier::getInstance().noteResetAudio();
	//跟hidl通信相关的服务
    mDevicesFactoryHal = DevicesFactoryHalInterface::create();
    mEffectsFactoryHal = EffectsFactoryHalInterface::create();
	//新建了一个MediaLogNotifier的线程，但是需要配合media.log服务运行，假如media.log没启动，该线程退出
    mMediaLogNotifier->run("MediaLogNotifier");

    setIsUseAudioWhaleHal(property_get_bool("ro.audio.whale_hal", false));
}
```

### AudioFlinger::onFirstRef

此后，由于AudioFlinger被强引用指向，调用onFirstRef方法

```c++
void AudioFlinger::onFirstRef()
{
//加锁防止同时访问，作用域为大括号内
    Mutex::Autolock _l(mLock);

    /* TODO: move all this work into an Init() function */
    char val_str[PROPERTY_VALUE_MAX] = { 0 };
    if (property_get("ro.audio.flinger_standbytime_ms", val_str, NULL) >= 0) {
        uint32_t int_val;
        if (1 == sscanf(val_str, "%u", &int_val)) {
            //设置了ro.audio.flinger_standbytime_ms时间
            mStandbyTimeInNsecs = milliseconds(int_val);
            ALOGI("Using %u mSec as standby time.", int_val);
        } else {
            //默认的standby时间为kDefaultStandbyTimeInNsecs，即3s。
            mStandbyTimeInNsecs = kDefaultStandbyTimeInNsecs;
            ALOGI("Using default %u mSec as standby time.",
                    (uint32_t)(mStandbyTimeInNsecs / 1000000));
        }
    }

    mMode = AUDIO_MODE_NORMAL;
	//将自身放在gAudioFlinger的全局变量中,方便客户端调用AudioFlinger服务
    gAudioFlinger = this;
}
```

系统默认在播放完音频后，经过mStandbyTimeInNsecs后会进入StandBy状态。

至于hidl相关，需要在方案里配置，当AudioFlinger创建hidl对象时:

```c++
//frameworks/av/services/audioflinger/AudioFlinger.cpp
AudioFlinger::AudioFlinger(){
    mDevicesFactoryHal = DevicesFactoryHalInterface::create();
    mEffectsFactoryHal = EffectsFactoryHalInterface::create();
}
```

```c++
////frameworks/av/media/libaudiohal/DevicesFactoryHalInterface.cpp
sp<DevicesFactoryHalInterface> DevicesFactoryHalInterface::create() {
    //从高到低获取服务是否存在，来确定使用哪一套方案
    if (hardware::audio::V5_0::IDevicesFactory::getService() != nullptr) {
        return V5_0::createDevicesFactoryHal();
    }
    if (hardware::audio::V4_0::IDevicesFactory::getService() != nullptr) {
        return V4_0::createDevicesFactoryHal();
    }
    if (hardware::audio::V2_0::IDevicesFactory::getService() != nullptr) {
        return V2_0::createDevicesFactoryHal();
    }
    return nullptr;
}

//frameworks/av/media/libaudiohal/DevicesFactoryHalInterface.cpp
sp<EffectsFactoryHalInterface> EffectsFactoryHalInterface::create() {
    if (hardware::audio::effect::V5_0::IEffectsFactory::getService() != nullptr) {
        return effect::V5_0::createEffectsFactoryHal();
    }
    if (hardware::audio::effect::V4_0::IEffectsFactory::getService() != nullptr) {
        return effect::V4_0::createEffectsFactoryHal();
    }
    if (hardware::audio::effect::V2_0::IEffectsFactory::getService() != nullptr) {
        return effect::V2_0::createEffectsFactoryHal();
    }
    return nullptr;
}
```

Hidl的配置一般在DeviceCommon.mk中配置

[device](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/device/)/[sprd](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/device/sprd/)/[sharkl3](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/device/sprd/sharkl3/)/[common](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/device/sprd/sharkl3/common/)/[DeviceCommon.mk](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/device/sprd/sharkl3/common/DeviceCommon.mk)

```
60 #audio hidl hal impl
61 PRODUCT_PACKAGES += \
62     android.hardware.audio@5.0-impl \
63     android.hardware.audio.effect@5.0-impl \
64     android.hardware.broadcastradio@1.0-impl \
65     android.hardware.soundtrigger@2.0-impl \
66     android.hardware.audio@2.0-service
```

Hidl的工作暂不分析。至此，AudioFlinger初始化完成。进入AudioPolicyService的初始化。

## AudioPolicyService::instantiate

### AudioPolicyService::onFirstRef

AudioPolicyService在服务中注册为“media.audio_policy”，其instantiate初始化与AudioFlinger如出一辙，因此可以直接到其构造函数进行分析:

```c++
//frameworks/av/services/audiopolicy/service/AudioPolicyService.cpp
AudioPolicyService::AudioPolicyService()
    : BnAudioPolicyService(), mpAudioPolicyDev(NULL), mpAudioPolicy(NULL),
      mAudioPolicyManager(NULL), mAudioPolicyClient(NULL), mPhoneState(AUDIO_MODE_INVALID)
{
}
```

构造方法仅仅通过初始化列表对成员变量进行了初始化，因此直接到onFirstRef进行分析:

```c++
void AudioPolicyService::onFirstRef()
{
    {
        Mutex::Autolock _l(mLock);
        //AudioPolicyService创建了2个AudioCommandThread线程，
        //发送音频配置命令到AudioFlinger中(step 1)
        // start audio commands thread
        mAudioCommandThread = new AudioCommandThread(String8("ApmAudio"), this);
        // start output activity command thread
        mOutputCommandThread = new AudioCommandThread(String8("ApmOutput"), this);
		//新建AudioPolicyClient对象，并传入到createAudioPolicyManger接口(step 2)
        mAudioPolicyClient = new AudioPolicyClient(this);
        //新建出AduioPolicyManager对象。(step 3)
        mAudioPolicyManager = createAudioPolicyManager(mAudioPolicyClient);
//sprd自定义的策略
#ifdef SPRD_CUSTOM_AUDIO_POLICY
        char value[PROPERTY_VALUE_MAX];
        int forced_val;
        /*SPRD for new policy, set the camera forced sound*/
        property_get("ro.camera.sound.forced", value, "1");
        forced_val = strtol(value, NULL, 0);
        ALOGV("ro.camera.sound.forced !forced_val=%d ",!forced_val);
        mAudioPolicyManager->setSystemProperty("ro.camera.sound.forced", !forced_val ? "0" : "1");
#endif
    }
    // load audio processing modules
    sp<AudioPolicyEffects>audioPolicyEffects = new AudioPolicyEffects();
    {
        Mutex::Autolock _l(mLock);
        mAudioPolicyEffects = audioPolicyEffects;
    }

#ifdef SPRD_CUSTOM_AUDIO_POLICY
    pthread_t uidpolicy;
    if (pthread_create(&uidpolicy, nullptr, UidPolicyThreadWrapper, this) != 0) {
        ALOGW("Failed to create thread for uid policy register.");
    }
    ALOGW("create thread for uid policy register.");

    pthread_t sensorPrivacyPolicy;
    if (pthread_create(&sensorPrivacyPolicy, nullptr, SensorPrivacyPolicyThreadWrapper, this) != 0) {
        ALOGW("Failed to create thread for sensor privacy policy register.");
    }
    ALOGW("create thread for sensor privacy policy register.");

#else
    mUidPolicy = new UidPolicy(this);
    mUidPolicy->registerSelf();

    mSensorPrivacyPolicy = new SensorPrivacyPolicy(this);
    mSensorPrivacyPolicy->registerSelf();
#endif
}
```

step 1: AudioCommandThread在threadLoop循环处理消息，处理的消息类型包括:

```c++
//frameworks/av/services/audiopolicy/service/AudioPolicyService.h
430      class AudioCommandThread : public Thread {
431          class AudioCommand;
432      public:
433  
434          // commands for tone AudioCommand
435          enum {
436              SET_VOLUME,
437              SET_PARAMETERS,
438              SET_VOICE_VOLUME,
439              STOP_OUTPUT,
440              RELEASE_OUTPUT,
441              CREATE_AUDIO_PATCH,
442              RELEASE_AUDIO_PATCH,
443              UPDATE_AUDIOPORT_LIST,
444              UPDATE_AUDIOPATCH_LIST,
445              CHANGED_AUDIOVOLUMEGROUP,
446              SET_AUDIOPORT_CONFIG,
447              DYN_POLICY_MIX_STATE_UPDATE,
448              RECORDING_CONFIGURATION_UPDATE,
449              SET_EFFECT_SUSPENDED,
450          };
    
//frameworks/av/services/audiopolicy/service/AudioPolicyService.cpp
1092  bool AudioPolicyService::AudioCommandThread::threadLoop()
1093  {
1094      nsecs_t waitTime = -1;
1095  
1096      mLock.lock();
1097      while (!exitPending())
1098      {
1099          sp<AudioPolicyService> svc;
1100          while (!mAudioCommands.isEmpty() && !exitPending()) {
1101              nsecs_t curTime = systemTime();
1102              // commands are sorted by increasing time stamp: execute them from index 0 and up
1103              if (mAudioCommands[0]->mTime <= curTime) {
1104                  sp<AudioCommand> command = mAudioCommands[0];
1105                  mAudioCommands.removeAt(0);
1106                  mLastCommand = command;
1107  
1108                  switch (command->mCommand) {
1109                  case SET_VOLUME: {
1110                      VolumeData *data = (VolumeData *)command->mParam.get();
1111                      ALOGV("AudioCommandThread() processing set volume stream %d, \
1112                              volume %f, output %d", data->mStream, data->mVolume, data->mIO);
1113                      mLock.unlock();
1114                      command->mStatus = AudioSystem::setStreamVolume(data->mStream,
1115                                                                      data->mVolume,
1116                                                                      data->mIO);
1117                      mLock.lock();
1118                      }break;
1119                  case SET_PARAMETERS: {
1120                      ParametersData *data = (ParametersData *)command->mParam.get();
1121                      ALOGV("AudioCommandThread() processing set parameters string %s, io %d",
1122                              data->mKeyValuePairs.string(), data->mIO);
1123                      mLock.unlock();
1124                      command->mStatus = AudioSystem::setParameters(data->mIO, data->mKeyValuePairs);
1125                      mLock.lock();
1126                      }break;
1127                  case SET_VOICE_VOLUME: {
1128                      VoiceVolumeData *data = (VoiceVolumeData *)command->mParam.get();
1129                      ALOGV("AudioCommandThread() processing set voice volume volume %f",
1130                              data->mVolume);
1131                      mLock.unlock();
1132                      command->mStatus = AudioSystem::setVoiceVolume(data->mVolume);
1133                      mLock.lock();
1134                      }break;
1135                  case STOP_OUTPUT: {
1136                      StopOutputData *data = (StopOutputData *)command->mParam.get();
1137                      ALOGV("AudioCommandThread() processing stop output portId %d",
1138                              data->mPortId);
1139                      svc = mService.promote();
1140                      if (svc == 0) {
1141                          break;
1142                      }
1143                      mLock.unlock();
1144                      svc->doStopOutput(data->mPortId);
1145                      mLock.lock();
1146                      }break;
1147                  case RELEASE_OUTPUT: {
1148                      ReleaseOutputData *data = (ReleaseOutputData *)command->mParam.get();
1149                      ALOGV("AudioCommandThread() processing release output portId %d",
1150                              data->mPortId);
1151                      svc = mService.promote();
1152                      if (svc == 0) {
1153                          break;
1154                      }
1155                      mLock.unlock();
1156                      svc->doReleaseOutput(data->mPortId);
1157                      mLock.lock();
1158                      }break;
1159                  case CREATE_AUDIO_PATCH: {
1160                      CreateAudioPatchData *data = (CreateAudioPatchData *)command->mParam.get();
1161                      ALOGV("AudioCommandThread() processing create audio patch");
1162                      sp<IAudioFlinger> af = AudioSystem::get_audio_flinger();
1163                      if (af == 0) {
1164                          command->mStatus = PERMISSION_DENIED;
1165                      } else {
1166                          mLock.unlock();
1167                          command->mStatus = af->createAudioPatch(&data->mPatch, &data->mHandle);
1168                          mLock.lock();
1169                      }
1170                      } break;
1171                  case RELEASE_AUDIO_PATCH: {
1172                      ReleaseAudioPatchData *data = (ReleaseAudioPatchData *)command->mParam.get();
1173                      ALOGV("AudioCommandThread() processing release audio patch");
1174                      sp<IAudioFlinger> af = AudioSystem::get_audio_flinger();
1175                      if (af == 0) {
1176                          command->mStatus = PERMISSION_DENIED;
1177                      } else {
1178                          mLock.unlock();
1179                          command->mStatus = af->releaseAudioPatch(data->mHandle);
1180                          mLock.lock();
1181                      }
1182                      } break;
1183                  case UPDATE_AUDIOPORT_LIST: {
1184                      ALOGV("AudioCommandThread() processing update audio port list");
1185                      svc = mService.promote();
1186                      if (svc == 0) {
1187                          break;
1188                      }
1189                      mLock.unlock();
1190                      svc->doOnAudioPortListUpdate();
1191                      mLock.lock();
1192                      }break;
1193                  case UPDATE_AUDIOPATCH_LIST: {
1194                      ALOGV("AudioCommandThread() processing update audio patch list");
1195                      svc = mService.promote();
1196                      if (svc == 0) {
1197                          break;
1198                      }
1199                      mLock.unlock();
1200                      svc->doOnAudioPatchListUpdate();
1201                      mLock.lock();
1202                      }break;
1203                  case CHANGED_AUDIOVOLUMEGROUP: {
1204                      AudioVolumeGroupData *data =
1205                              static_cast<AudioVolumeGroupData *>(command->mParam.get());
1206                      ALOGV("AudioCommandThread() processing update audio volume group");
1207                      svc = mService.promote();
1208                      if (svc == 0) {
1209                          break;
1210                      }
1211                      mLock.unlock();
1212                      svc->doOnAudioVolumeGroupChanged(data->mGroup, data->mFlags);
1213                      mLock.lock();
1214                      }break;
1215                  case SET_AUDIOPORT_CONFIG: {
1216                      SetAudioPortConfigData *data = (SetAudioPortConfigData *)command->mParam.get();
1217                      ALOGV("AudioCommandThread() processing set port config");
1218                      sp<IAudioFlinger> af = AudioSystem::get_audio_flinger();
1219                      if (af == 0) {
1220                          command->mStatus = PERMISSION_DENIED;
1221                      } else {
1222                          mLock.unlock();
1223                          command->mStatus = af->setAudioPortConfig(&data->mConfig);
1224                          mLock.lock();
1225                      }
1226                      } break;
1227                  case DYN_POLICY_MIX_STATE_UPDATE: {
1228                      DynPolicyMixStateUpdateData *data =
1229                              (DynPolicyMixStateUpdateData *)command->mParam.get();
1230                      ALOGV("AudioCommandThread() processing dyn policy mix state update %s %d",
1231                              data->mRegId.string(), data->mState);
1232                      svc = mService.promote();
1233                      if (svc == 0) {
1234                          break;
1235                      }
1236                      mLock.unlock();
1237                      svc->doOnDynamicPolicyMixStateUpdate(data->mRegId, data->mState);
1238                      mLock.lock();
1239                      } break;
1240                  case RECORDING_CONFIGURATION_UPDATE: {
1241                      RecordingConfigurationUpdateData *data =
1242                              (RecordingConfigurationUpdateData *)command->mParam.get();
1243                      ALOGV("AudioCommandThread() processing recording configuration update");
1244                      svc = mService.promote();
1245                      if (svc == 0) {
1246                          break;
1247                      }
1248                      mLock.unlock();
1249                      svc->doOnRecordingConfigurationUpdate(data->mEvent, &data->mClientInfo,
1250                              &data->mClientConfig, data->mClientEffects,
1251                              &data->mDeviceConfig, data->mEffects,
1252                              data->mPatchHandle, data->mSource);
1253                      mLock.lock();
1254                      } break;
1255                  case SET_EFFECT_SUSPENDED: {
1256                      SetEffectSuspendedData *data = (SetEffectSuspendedData *)command->mParam.get();
1257                      ALOGV("AudioCommandThread() processing set effect suspended");
1258                      sp<IAudioFlinger> af = AudioSystem::get_audio_flinger();
1259                      if (af != 0) {
1260                          mLock.unlock();
1261                          af->setEffectSuspended(data->mEffectId, data->mSessionId, data->mSuspended);
1262                          mLock.lock();
1263                      }
1264                      } break;
1265  
1266                  default:
1267                      ALOGW("AudioCommandThread() unknown command %d", command->mCommand);
1268                  }
1269                  {
1270                      Mutex::Autolock _l(command->mLock);
1271                      if (command->mWaitStatus) {
1272                          command->mWaitStatus = false;
1273                          command->mCond.signal();
1274                      }
1275                  }
1276                  waitTime = -1;
1277                  // release mLock before releasing strong reference on the service as
1278                  // AudioPolicyService destructor calls AudioCommandThread::exit() which
1279                  // acquires mLock.
1280                  mLock.unlock();
1281                  svc.clear();
1282                  mLock.lock();
1283              } else {
1284                  waitTime = mAudioCommands[0]->mTime - curTime;
1285                  break;
1286              }
1287          }
1288  
1289          // release delayed commands wake lock if the queue is empty
1290          if (mAudioCommands.isEmpty()) {
1291              release_wake_lock(mName.string());
1292          }
1293  
1294          // At this stage we have either an empty command queue or the first command in the queue
1295          // has a finite delay. So unless we are exiting it is safe to wait.
1296          if (!exitPending()) {
1297              ALOGV("AudioCommandThread() going to sleep");
1298              if (waitTime == -1) {
1299                  mWaitWorkCV.wait(mLock);
1300              } else {
1301                  mWaitWorkCV.waitRelative(mLock, waitTime);
1302              }
1303          }
1304      }
1305      // release delayed commands wake lock before quitting
1306      if (!mAudioCommands.isEmpty()) {
1307          release_wake_lock(mName.string());
1308      }
1309      mLock.unlock();
1310      return false;
1311  }
```

为了让AudioCommandThread处理消息，通过sendCommmand将command填充到mAudioCommands中，并随后通过mWaitWorkCV.signal()唤醒线程,线程首先通过mWaitWorkCV.wait(mLock)进行等待，当被唤醒后，就会开始处理，最终将给到AudioFlinger处理.

step 2: AudioPolicyClient继承了AudioPolicyClientInterface，基类AudioPolicyClientInterface定义在/hardware/libhardware_legacy/include/hardware_legacy/AudioPolicyInterface.h文件中，定义了hal层的重要接口,如下是部分接口展示:

```c
///hardware/libhardware_legacy/include/hardware_legacy/AudioPolicyInterface.h
//加载音频设备的描述文件audio_policy.conf    
virtual audio_module_handle_t loadHwModule(const char *name) = 0;
//打开音频输出通道 
virtual audio_io_handle_t openOutput(audio_module_handle_t module,
                                     audio_devices_t *pDevices,
                                     uint32_t *pSamplingRate,
                                     audio_format_t *pFormat,
                                     audio_channel_mask_t *pChannelMask,
                                     uint32_t *pLatencyMs,
                                     audio_output_flags_t flags,
                                     const audio_offload_info_t *offloadInfo = NULL) = 0;

//关闭音频输出通道
virtual status_t closeOutput(audio_io_handle_t output) = 0;
... 
//打开音频输入通道 
virtual audio_io_handle_t openInput(audio_module_handle_t module,
                                    audio_devices_t *pDevices,
                                    uint32_t *pSamplingRate,
                                    audio_format_t *pFormat,
                                    audio_channel_mask_t *pChannelMask) = 0;
virtual status_t closeInput(audio_io_handle_t input) = 0;
//设置音频音量 
virtual status_t setStreamVolume(AudioSystem::stream_type stream, float volume, audio_io_handle_t output, int delayMs = 0) = 0;
... 
```
AudioPolicyClient作为子类，需要实现这些方法，但实际的操作不是它来完成，而是AudioFlinger，如loadHwModule的实现:

```c++
//frameworks/av/services/audiopolicy/service/AudioPolicyClientImpl.cpp
audio_module_handle_t AudioPolicyService::AudioPolicyClient::loadHwModule(const char *name)
{
    //通过get_audio_flinger获取AudioFlinger
    sp<IAudioFlinger> af = AudioSystem::get_audio_flinger();
    if (af == 0) {
        return AUDIO_MODULE_HANDLE_NONE;
    }
    //最终调用到AudioFlinger的loadHwModule完成
    return af->loadHwModule(name);
}
```

step 3: 回到AudioPolicyService的流程，新建完AudioPolicyClient后作为参数，传到了方法createAudioPolicyManager中,其定义在frameworks/av/services/audiopolicy/manager/AudioPolicyFactory.cpp中,sprd在AudioPolicyManager继承了AudioPolicyManager并重写了这个方法

```c++
//vendor/sprd/modules/audio/normal/newapm/Q.x/AudioPolicyManagerSPRD.cpp
37  extern "C" AudioPolicyInterface* createAudioPolicyManager(
38          AudioPolicyClientInterface *clientInterface)
39  {
40      return new AudioPolicyManagerSPRD(clientInterface);
41  }
```

AudioPolicyManagerSPRD初始化会先调用父类构造AudioPolicyManager：：AudioPolicyManager

```c++
//frameworks/av/services/audiopolicy/manager/AudioPolicyFactory.cpp
extern "C" AudioPolicyInterface* createAudioPolicyManager(
        AudioPolicyClientInterface *clientInterface)
{
    //实质是新建了一个AudioPolicyManager对象。
    return new AudioPolicyManager(clientInterface);
}
```

```c++
//frameworks/av/services/audiopolicy/managerdefault/AudioPolicyManager.cpp
AudioPolicyManager::AudioPolicyManager(AudioPolicyClientInterface *clientInterface)
        : AudioPolicyManager(clientInterface, false /*forTesting*/)
{
    loadConfig();
    initialize();
}
```

## AudioPolicyManager::loadConfig

adb shell dumpsys media.audio_policy



# AudioTrack分析:

## dump

http://shexsvn01/svn/CPT/SW_FAE/Android/05-Sample%20code/Multi-Media/audiotrackdump.rar

## java API概述

Android SDK 提供了3套音频播放的API，分别是：MediaPlayer，SoundPool，AudioTrack，关于它们的区别可以看这篇文章：[《Intro to the three Android Audio APIs》](http://www.wiseandroid.com/post/2010/07/13/Intro-to-the-three-Android-Audio-APIs.aspx)，简单来说，MediaPlayer 更加适合在后台长时间播放本地音乐文件或者在线的流式资源; SoundPool 则适合播放比较短的音频片段，比如游戏声音、按键声、铃声片段等等，它可以同时播放多个音频; 而 AudioTrack 则更接近底层，提供了非常强大的控制能力，支持低延迟播放，适合流媒体和VoIP语音电话等场景。

| API         | 音频文件                                           | 架构关系                                                     |                                                      |
| ----------- | -------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- |
| MediaPlayer | 播放多种格式的音源，如 mp3、flac、wma、ogg、wav 等 | 在 Native 层会创建对应的音频解码器和一个 AudioTrack，解码后的数据交由 AudioTrack 输出 | MediaPlayer 的应用场景更广，一般情况下使用它也更方便 |
| AudioTrack  | 播放解码后的 PCM 数据流                            | AudioTrack 输出                                              | 对声音时延要求非常苛刻的应用场景                     |

## SampleCode使用

```java
            //建造音频格式
            AudioFormat audioFormat = new AudioFormat.Builder().build();
            //建造音频属性
            AudioAttributes audioAttributes = new AudioAttributes.Builder().build();
            //创建播放音频最小buffer
            int minBufferSize = AudioTrack.getMinBufferSize(44100,
                    AudioFormat.CHANNEL_IN_STEREO,//声道数：双声道
                    AudioFormat.ENCODING_PCM_16BIT);//采样精度：16比特
            //创建audiotrack
            audioTrack = new AudioTrack(audioAttributes, audioFormat, minBufferSize,
                    AudioTrack.MODE_STREAM, AudioManager.AUDIO_SESSION_ID_GENERATE);
            byte[] byteBuffer = new byte[minBufferSize];
			...
            //开始播放
            audioTrack.play();
         	...
            //调用write写数据
            audioTrack.write(byteBuffer, 0, i);
			//停止和释放
            audioTrack.stop();
            audioTrack.release();
```

### 音频属性

音频播放器支持定义音频系统如何处理指定来源的导向、音量和焦点决策的属性。应用可以将属性附加到音频播放上，然后将音频源的属性传递给框架，此时音频系统会使用这些属性做出混音决策并将系统状态通知给应用。

示例:AudioAttributes.Builder` 定义了将由一个新的 `AudioTrack` 实例使用的 `AudioAttributes：

```java
AudioTrack myTrack = new AudioTrack( new AudioAttributes.Builder().setUsage(AudioAttributes.USAGE_MEDIA)  .setContentType(AudioAttributes.CONTENT_TYPE_MUSIC).build(), myFormat, myBuffSize, AudioTrack.MODE_STREAM, mySession);
```

### 两种数据传输模式

| Transfer Mode | Description                                                  |
| ------------- | ------------------------------------------------------------ |
| MODE_STATIC   | 应用进程将回放数据一次性付给 AudioTrack，适用于数据量小、时延要求高的场景 |
| MODE_STREAM   | 用进程需要持续调用 write() 写数据到 FIFO，写数据时有可能遭遇阻塞（等待 AudioFlinger::PlaybackThread 消费之前的数据），基本适用所有的音频场景 |

注意：如果采用STATIC模式，须先调用write写数据，然后再调用play。

以下分析主要基于MODE_STREAM模式分析。

### 音频流

Android 9以上不推荐使用，

区分声音类型，各种声音分别控制

c++层-[system](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/)/[media](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/)/[audio](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/)/[include](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/)/[system](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/system/)/[audio-base.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/system/audio-base.h)

```c++
20  typedef enum {
21      AUDIO_STREAM_DEFAULT = -1, // (-1)
..
33      AUDIO_STREAM_ACCESSIBILITY = 10,
```

java层-media/java/android/media/AudioSystem.java

```java
/** Used to identify the default audio stream volume */
public static final int STREAM_DEFAULT = -1;
/** Used to identify the volume of audio streams for phone calls */
...
/** Used to identify the volume of audio streams for accessibility prompts */
public static final int STREAM_ACCESSIBILITY = 10;
```

注意：Android 9 弃用了以上用于汽车的流类型，详见车载音频相关说明。

### getMinBufferSize流程

它配置的是 AudioTrack 内部的音频缓冲区的大小，该缓冲区的值不能低于一帧“音频帧”（Frame）的大小，一帧音频帧的大小计算如下：

int size = 采样率 x 位宽 x 采样时间 x 通道数

采样时间一般取 2.5ms~120ms 之间，由厂商或者具体的应用决定，我们其实可以推断，每一帧的采样时间取得越短，产生的延时就应该会越小，当然，碎片化的数据也就会越多。

在Android开发中，AudioTrack 类提供了一个帮助你确定这个 bufferSizeInBytes 的函数，原型如下：

int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat);

不同的厂商的底层实现是不一样的，但无外乎就是根据上面的计算公式得到一帧的大小，音频缓冲区的大小则必须是一帧大小的2～N倍。

实际开发中，强烈建议由该函数计算出需要传入的 bufferSizeInBytes，而不是自己手动计算。

getMinBufSize会综合考虑硬件的情况（诸如是否支持采样率，硬件本身的延迟情况等）后，得出一个最小缓冲区的大小。一般我们分配的缓冲大小会是它的整数倍。

![image-20200819163244651](C:\Users\hursion.zhang\AppData\Roaming\Typora\typora-user-images\image-20200819163244651.png)



## Java空间的分析

包括JNI层，主要涉及类：

media/java/android/media/AudioTrack.java

core/jni/android_media_AudioTrack.cpp

android_media_AudioTrack.cpp::AudioTrackJniStorage-通过Binder共享内存来分配内存

### 创建AudioTrack

google推荐使用通过静态内部类Builder.build来创建AudioTrack对象，建造者模式可以使传入的参数更方便管理，只需要关注我们需要传入的参数，当然我们也可以通过共有的构造器创建对象，以下是以streamType作为参数的构造，streamType会传入AudioAttributes并调用形参AudioAttributes的构造器，

```java
public AudioTrack(int streamType, int sampleRateInHz, int channelConfig, int audioFormat,
        int bufferSizeInBytes, int mode, int sessionId)
throws IllegalArgumentException {
    // mState already == STATE_UNINITIALIZED
    this((new AudioAttributes.Builder())
                .setLegacyStreamType(streamType)
                .build(),
```

但是不管是Builder还是公有构造器最终都会调用以下私有构造器

```java
private AudioTrack(AudioAttributes attributes, AudioFormat format, int bufferSizeInBytes,
        int mode, int sessionId, boolean offload)//音频offload模式（音频由dsp解码，不走omx框架）
                throws IllegalArgumentException {
    ...
    //前面主要是状态、属性，通道数，采样率等检查
    // native initialization，native_setup对应的JNI层函数是android_media_AudioTrack_native_setup
    int initResult = native_setup(new WeakReference<AudioTrack>(this), mAttributes,
            sampleRate, mChannelMask, mChannelIndexMask, mAudioFormat,
            mNativeBufferSizeInBytes, mDataLoadMode, session, 0 /*nativeTrackInJavaObj*/  
	...
}
```

### native_setup

JNI层函数是android_media_AudioTrack.cpp::android_media_AudioTrack_native_setup

主要做三件事，

1.创建AudioTrackJniStorage对象，保存AudioTrack callback数据

2.使用无参构造创建native AudioTrack

3.根据传输模式分别调用set函数，把Java层的参数传进去

```c++
static jint
android_media_AudioTrack_setup(JNIEnv *env, jobject thiz, jobject weak_this, jobject jaa,
        jintArray jSampleRate, jint channelPositionMask, jint channelIndexMask,
        jint audioFormat, jint buffSizeInBytes, jint memoryMode, jintArray jSession,
        jlong nativeAudioTrack, jboolean offload) {
	...
	//声明AudioTrackJniStorage对象，它保存了一些信息
    AudioTrackJniStorage* lpJniStorage = NULL;
  	....	
        // create the native AudioTrack object
        lpTrack = new AudioTrack();
		...
        // initialize the callback information:
        // this data will be passed with every AudioTrack callback
        lpJniStorage = new AudioTrackJniStorage();
        lpJniStorage->mCallbackData.audioTrack_class = (jclass)env->NewGlobalRef(clazz);
		...
		//根据传输模式选择
        // initialize the native AudioTrack object
        status_t status = NO_ERROR;
        switch (memoryMode) {
        case MODE_STREAM:
            status = lpTrack->set(
                    AUDIO_STREAM_DEFAULT,// stream type, but more info conveyed in paa (last argument)
                    sampleRateInHertz,
                    format,// word length, PCM
                    nativeChannelMask,
                    offload ? 0 : frameCount,
                    offload ? AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD : AUDIO_OUTPUT_FLAG_NONE,
                    audioCallback, &(lpJniStorage->mCallbackData),//callback, callback data (user)
                    0,// notificationFrames == 0 since not using EVENT_MORE_DATA to feed the AudioTrack
                    0,// shared mem//共享内存,STREAM模式下为空。实际使用的共享内存由AF创建
                    true,// thread can call Java
                    sessionId,// audio session ID
                    offload ? AudioTrack::TRANSFER_SYNC_NOTIF_CALLBACK : AudioTrack::TRANSFER_SYNC,
                    offload ? &offloadInfo : NULL,
                    -1, -1,                       // default uid, pid values
                    paa.get());

            break;

        case MODE_STATIC:
            // AudioTrack is using shared memory
			//如果是static模式，需要先创建共享内存
            if (!lpJniStorage->allocSharedMem(buffSizeInBytes)) {
                ALOGE("Error creating AudioTrack in static mode: error creating mem heap base");
                goto native_init_failure;
            }

            status = lpTrack->set(
                    AUDIO_STREAM_DEFAULT,// stream type, but more info conveyed in paa (last argument)
                    sampleRateInHertz,
                    format,// word length, PCM
                    nativeChannelMask,
                    frameCount,
                    AUDIO_OUTPUT_FLAG_NONE,
                    audioCallback, &(lpJniStorage->mCallbackData),//callback, callback data (user));
                    0,// notificationFrames == 0 since not using EVENT_MORE_DATA to feed the AudioTrack
                    lpJniStorage->mMemBase,// shared mem//STATIC模式下，需要传递该共享内存
                    true,// thread can call Java
                    sessionId,// audio session ID
                    AudioTrack::TRANSFER_SHARED,
                    NULL,                         // default offloadInfo
                    -1, -1,                       // default uid, pid values
                    paa.get());
            break;
....
    } 
...
}
```

### play、write、stop、release

* play-->startImpl-->native_start-->android_media_AudioTrack_start-->(lpTrack->start())

* write根据参数有七个同名方法，按byte[]写入流程

write-->native_write_byte-->android_media_AudioTrack_writeArray->writeToTrack

* stop-->native_stop->android_media_AudioTrack_stop-->(lpTrack->stop())

* release-->native_release-->android_media_AudioTrack_release-->(delete pJniStorage)

###  AudioTrack（Java空间）的分析总结

AudioTrack在JNI层使用了Native的AudioTrack对象，总结一下调用Native对象的流程：

· new一个AudioTrack，使用无参的构造函数。

· 调用set函数，把Java层的参数传进去，另外还设置了一个audiocallback回调函数。

· 调用了AudioTrack的start函数。

· 调用AudioTrack的write函数。

· 工作完毕后，调用stop。

· 最后就是Native对象的delete。

## Native空间的分析

# AudioRecord分析：

 (1) Java层调用Android的SDK中的API实例化一个AudioRecord对象

　　-》frameworks\av\media\libaudioclient\AudioRecord.cpp -》 AudioRecord::AudioRecord

 (2) 设置相应参数 

　　-》 mStatus = set(inputSource, sampleRate, format, channelMask, frameCount, cbf, user,
      　　　　　　　　notificationFrames, false /*threadCanCallJava*/, sessionId, transferType, flags,
      　　　　　 　　uid, pid, pAttributes);

 (3)打开录音接口

  　-》　// create the IAudioRecord
  　 　　status_t status = openRecord_l(0 /*epoch*/, mOpPackageName);

 (4) 获取输入设备属性：

 　　　status = AudioSystem::getInputForAttr(&mAttributes, &input,
                    mSessionId,
                    // FIXME compare to AudioTrack
                    mClientPid,
                    mClientUid,
                    &config,
                    mFlags, mSelectedDeviceId, &mPortId);

　　　　 其中是通过获取audio_policy_service建立binder接口:

　　　　 // establish binder interface to AudioPolicy service
　　　　 const sp<IAudioPolicyService> AudioSystem::get_audio_policy_service() 

 (5) 调用AudioPolicyManager的接口 ： \android-8.0.0_r4\frameworks\av\services\audiopolicy\managerdefault\AudioPolicyManager.cpp

　　　aps->getInputForAttr -》 AudioPolicyManager::getInputForAttr

 (6) 根据app传下的参数获取对应的设备类型：

　　device = getDeviceAndMixForInputSource(inputSource, &policyMix); 

　　-》Engine::getDeviceForInputSource    (\android-8.0.0_r4\frameworks\av\services\audiopolicy\enginedefault\src\Engine.cpp)

​    一般录音软件是：AUDIO_SOURCE_MIC ，google 语音引擎是：AUDIO_SOURCE_VOICE_RECOGNITION

　　 再根据当前系统支持的输入设备返回对应的录音设备：（默认的内置mic就是 AUDIO_DEVICE_IN_BUILTIN_MIC）

```
 if (mForceUse[AUDIO_POLICY_FORCE_FOR_RECORD] == AUDIO_POLICY_FORCE_BT_SCO &&
                availableDeviceTypes & AUDIO_DEVICE_IN_BLUETOOTH_SCO_HEADSET) {
            device = AUDIO_DEVICE_IN_BLUETOOTH_SCO_HEADSET;
        } else if (availableDeviceTypes & AUDIO_DEVICE_IN_WIRED_HEADSET) {
            device = AUDIO_DEVICE_IN_WIRED_HEADSET;
        } else if (availableDeviceTypes & AUDIO_DEVICE_IN_USB_HEADSET) {
            device = AUDIO_DEVICE_IN_USB_HEADSET;
        } else if (availableDeviceTypes & AUDIO_DEVICE_IN_USB_DEVICE) {
            device = AUDIO_DEVICE_IN_USB_DEVICE;
        } else if (availableDeviceTypes & AUDIO_DEVICE_IN_BUILTIN_MIC) {
            device = AUDIO_DEVICE_IN_BUILTIN_MIC;
        }
```

PS：通过 **dumpsys media.audio_policy** 指令查看当前系统所支持的设备模块及类型。

可通过frameworks\av\services\audiopolicy目录里面 audio_policy.conf或者audio_policy_configuration.xml配置设备加载，

使用.conf还是.xml取决于frameworks/av/services/audiopolicy/Android.mk 通过宏USE_XML_AUDIO_POLICY_CONF - 1 : 使用 .xml , 0: 使用 .conf。


 (7) 根据返回的device类型获取对应录音设备：

```
 　　　　*input = getInputForDevice(device, address, session, uid, inputSource,
                           　　    config->sample_rate, config->format, config->channel_mask, flags,
                            　　   policyMix);
```

 (8) 调用getInputProfile函数根据传进来的声音采样率、声音格式、通道掩码等参数与获得的设备支持的Input Profile比较返回一个与设备Profile匹配的IOProfile-》

```
　　　　　profile = getInputProfile(device, address,
                                  　　　　profileSamplingRate, profileFormat, profileChannelMask,
                                 　　　　 profileFlags);
```

 (9) 根据返回的profile调用初始化时加载好的client接口：

```
status_t status = mpClientInterface->openInput(profile->getModuleHandle(),
                                                   &input,
                                                   &config,
                                                   &device,
                                                   address,
                                                   halInputSource,
                                                   profilelags);
```

PS: mpClientInterface是由AudioPolicyService中加载对应模块传递过来：

```
void AudioPolicyService::onFirstRef()
{
    {
        Mutex::Autolock _l(mLock);

        // start tone playback thread
        mTonePlaybackThread = new AudioCommandThread(String8("ApmTone"), this);
        // start audio commands thread
        mAudioCommandThread = new AudioCommandThread(String8("ApmAudio"), this);
        // start output activity command thread
        mOutputCommandThread = new AudioCommandThread(String8("ApmOutput"), this);

        mAudioPolicyClient = new AudioPolicyClient(this);
        mAudioPolicyManager = createAudioPolicyManager(mAudioPolicyClient); //对应的模块加载放在AudioPolicyManager的构造函数中   
    }
    // load audio processing modules
    sp<AudioPolicyEffects>audioPolicyEffects = new AudioPolicyEffects();
    {
        Mutex::Autolock _l(mLock);
        mAudioPolicyEffects = audioPolicyEffects;
    }
}
```

 AudioPolicyManager构造函数中根据宏定义判断通过audio_policy.conf还是audio_policy_configuration.xml解析加载对应的so:

```
#ifdef USE_XML_AUDIO_POLICY_CONF
    mVolumeCurves = new VolumeCurvesCollection();
    AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices,
                             mDefaultOutputDevice, speakerDrcEnabled,
                             static_cast<VolumeCurvesCollection *>(mVolumeCurves));
    if (deserializeAudioPolicyXmlConfig(config) != NO_ERROR) {
#else
    mVolumeCurves = new StreamDescriptorCollection();
    AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices,
                             mDefaultOutputDevice, speakerDrcEnabled);
    if ((ConfigParsingUtils::loadConfig(AUDIO_POLICY_VENDOR_CONFIG_FILE, config) != NO_ERROR) &&
            (ConfigParsingUtils::loadConfig(AUDIO_POLICY_CONFIG_FILE, config) != NO_ERROR)) {
#endif
```

 加载模块:

```
        mHwModules[i]->mHandle = mpClientInterface->loadHwModule(mHwModules[i]->getName());
        if (mHwModules[i]->mHandle == 0) {
            ALOGW("could not open HW module %s", mHwModules[i]->getName());
            continue;
        }
```

具体加载过程：

AudioFlinger::loadHwModule_l 

-》mDevicesFactoryHal->openDevice(name, &dev);

　　-》DevicesFactoryHalHidl::openDevice(const char *name, sp<DeviceHalInterface> *device) 

　　　　-》DevicesFactory::openDevice(IDevicesFactory::Device device, openDevice_cb _hidl_cb) //对应`hardware/interfaces/audio/2.0/`目录下IDevicesFactory.hal所描述接口

　　　　　　-》loadAudioInterface(moduleName, &halDevice);

　　　　　　　　-》DevicesFactory::loadAudioInterface(const char *if_name, audio_hw_device_t **dev)

```
 　 //加载so,后面最终通过dlopen加载了/system/lib/hw/下对应的so。　　rc = hw_get_module_by_class(AUDIO_HARDWARE_MODULE_ID, if_name, &mod);
    if (rc) {
        ALOGE("%s couldn't load audio hw module %s.%s (%s)", __func__,
                AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc));
        goto out;
    }　　 //实际调用到了audio_hw.c中adev_open()，只会被调用一次，也就是给硬件模块中的函数指针赋值open()。
    rc = audio_hw_device_open(mod, dev);
    if (rc) {
        ALOGE("%s couldn't open audio hw device in %s.%s (%s)", __func__,
                AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc));
        goto out;
    }
    if ((*dev)->common.version < AUDIO_DEVICE_API_VERSION_MIN) {
        ALOGE("%s wrong audio hw device version %04x", __func__, (*dev)->common.version);
        rc = -EINVAL;
        audio_hw_device_close(*dev);
        goto out;
    }
```

 (10) 再回到AudioPolicyManager::getInputForDevice 中的 mpClientInterface->openInput调用流程：

　　　　AudioFlinger::openInput -》 

　　　　　　AudioFlinger::openInput_l -》

　　　　 　 　　sp<DeviceHalInterface> inHwHal = inHwDev->hwDevice();

　　　　 　　  ...

　　　　 　　  inHwHal->openInputStream -》

　　　　  　　　　　DeviceHalHidl::openInputStream

```
 Return<void> ret = mDevice->openInputStream(
            handle,
            hidlDevice,
            hidlConfig,
            AudioInputFlag(flags),
            AudioSource(source),
            [&](Result r, const sp<IStreamIn>& result, const AudioConfig& suggestedConfig) {
                retval = r;
                if (retval == Result::OK) {
                    *inStream = new StreamInHalHidl(result); //audio_hw.c中adev_open_input_stream的参数stream_in在这里创建并传入
                }
                HidlUtils::audioConfigToHal(suggestedConfig, config);
            });
    return processReturn("openInputStream", ret, retval);
```

-》最终调用到了audio_hw.c中的：

```
static int adev_open_input_stream(struct audio_hw_device *dev,
                                  audio_io_handle_t handle,
                                  audio_devices_t devices,
                                  struct audio_config *config,
                                  struct audio_stream_in **stream_in,
                                  audio_input_flags_t flags,
                                  const char *address __unused,
                                  audio_source_t source )
```

adev_open_input_stream中对sp<StreamInHalInterface> *inStream指针的各成员进行赋值，进一步调用底层的接口获取音频数据。

PS：默认使用上层传下来的config参数，也可以手动更新stream_in的参数，比如 采样率：stream_in->config.rate，自己实现数据获取接口：stream_in->stream.read = my_read 等;





















# 音频路由策略

什么是音频路由？如何实现音频路由？如何定制音频路由？
什么是音频路由？音频无非就是把音频数据流放到指定的声卡上，然后进行播放。那么，音频路由就是要解决把某种类型的音频流放到那种声卡进行播放的策略。
为什么要这样做？因为现在的Android支持多声卡，并且Android本质有各种类型的音频流。这么多的音频流和声卡需要被有效管理起来，所以需要音频路由来实现。什么是音频流？这是对上层应用场景的概念，诸如铃声，多媒体音乐，TTS播报等，，某些音频流需要在不同的声卡设备上播放，从而满足不同的需求。你可能会问，手机不就一个喇叭吗，还需要这么复杂。下面我们来看一个场景，小明插着耳机听着音乐，此时他女朋友小华给他打了个电话。这个时候，铃声声音会同时从耳机和手机喇叭出 来，以保证小明能晓得有来电。如果单单是从耳机输出，当声音很小的时候那可能会被忽略掉（那可能是要被扒成皮的๑乛◡乛๑ ）。这这只是很简单的小场景。

## Android设备的定义

c++层-[system](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/)/[media](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/)/[audio](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/)/[include](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/)/[system](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/system/)/[audio-base.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/system/media/audio/include/system/audio-base.h)

```c++
326  enum {
327      AUDIO_DEVICE_NONE                          = 0x0u,
328      AUDIO_DEVICE_BIT_IN                        = 0x80000000u,
329      AUDIO_DEVICE_BIT_DEFAULT                   = 0x40000000u,
330  
....
389      AUDIO_DEVICE_IN_HDMI_ARC                   = 0x88000000u, // BIT_IN | 0x8000000
390      AUDIO_DEVICE_IN_ECHO_REFERENCE             = 0x90000000u, // BIT_IN | 0x10000000
391      AUDIO_DEVICE_IN_DEFAULT                    = 0xC0000000u, // BIT_IN | BIT_DEFAULT
392  };
```

java层-media/java/android/media/AudioSystem.java

```java
public static final int DEVICE_NONE = 0x0;
// reserved bits
public static final int DEVICE_BIT_IN = 0x80000000;
public static final int DEVICE_BIT_DEFAULT = 0x40000000;
// output devices, be sure to update AudioManager.java also
@UnsupportedAppUsage
...
/** UNISOC: bug687987, add FM devices: headset and speaker. @{ */
public static final int DEVICE_OUT_FM_HEADSET = 0x10000000;
public static final int DEVICE_OUT_FM_SPEAKER = 0x20000000;
/** @} */
...


```

```java
// input devices
@UnsupportedAppUsage
public static final int DEVICE_IN_COMMUNICATION = DEVICE_BIT_IN | 0x1;
@UnsupportedAppUsage
public static final int DEVICE_IN_AMBIENT = DEVICE_BIT_IN | 0x2;
...
public static final int DEVICE_IN_ECHO_REFERENCE = DEVICE_BIT_IN | 0x10000000;
```

### 定义

策略可以决定输出设备

[hardware](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/hardware/)/[libhardware_legacy](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/hardware/libhardware_legacy/)/[include](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/hardware/libhardware_legacy/include/)/[hardware_legacy](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/hardware/libhardware_legacy/include/hardware_legacy/)/[AudioPolicyManagerBase.h](http://10.0.1.79:8081/xref/sprdroid10_trunk_19c/hardware/libhardware_legacy/include/hardware_legacy/AudioPolicyManagerBase.h)

```c++
153          enum routing_strategy {
154              STRATEGY_MEDIA,
155              STRATEGY_PHONE,
156              STRATEGY_SONIFICATION,
157              STRATEGY_SONIFICATION_RESPECTFUL,
158              STRATEGY_DTMF,
159              STRATEGY_ENFORCED_AUDIBLE,
160              NUM_STRATEGIES
161          };
```

**既然是分析音频输出设备，我们首先需要知道当前手机支持的音频输出设备有哪些**？

```
adb shell dumpsys media.audio_policy > Desktop\8541e-audio-policy.txt
```

```xml
 Config source: /vendor/etc/audio_policy_configuration.xml
- Available output devices:
  Device 1:
  - id:  2
  - tag name: Earpiece
  - type: AUDIO_DEVICE_OUT_EARPIECE                       
  - Profiles:
      Profile 0:
          - format: AUDIO_FORMAT_PCM_16_BIT
          - sampling rates:44100
          - channel masks:0x0010
  Device 2:
  - id:  3
  - tag name: Speaker
  - type: AUDIO_DEVICE_OUT_SPEAKER                        
  - Profiles:
      Profile 0:
          - format: AUDIO_FORMAT_PCM_16_BIT
          - sampling rates:44100
          - channel masks:0x0003
```



平台将 `AudioPolicyManager.cpp` 拆分为多个模块，使其易于维护和配置。`frameworks/av/services/audiopolicy` 的组织包括以下模块：

| 模块                  | 说明                                                         |
| :-------------------- | :----------------------------------------------------------- |
| `/managerdefault`     | 包含所有应用通用的常规接口和行为实现。类似于剥离了引擎功能和通用概念的 `AudioPolicyManager.cpp`。 |
| `/common`             | 定义基类（例如，输入输出音频流配置文件、音频设备描述符、音频补丁程序和音频端口等的数据结构）。该模块之前在 `AudioPolicyManager.cpp` 中定义。 |
| `/engine`             | 实现规则，这些规则定义应将哪些设备和音量用于指定使用情形。该模块会实现标准接口（包含通用部分），例如，为指定的播放或捕获使用情形获取适当的设备，或设置可以改变导向选择的已连接设备或外部状态（即强制使用的调用状态）。两种版本（自定义版本和默认版本）中都提供该模块；使用编译选项 `USE_CONFIGURABLE_AUDIO_POLICY` 进行选择。 |
| `/engineconfigurable` | 依赖参数框架的政策引擎实现（请参阅下文）。配置基于参数框架，相关政策由 XML 文件定义。 |
| `/enginedefault`      | 基于以前的 Android 音频政策管理器实现的政策引擎实现。这是默认模块，包含 Nexus 和 AOSP 实现对应的硬编码规则。 |
| `/service`            | 包含 Binder 接口、线程和锁定实现（包含连接框架其余部分的接口）。 |



# 车载音频-todo

https://source.android.google.cn/devices/automotive/audio?hl=zh-cn

# NewMusic-todo



# 编解码-todo

https://source.android.google.cn/devices/media#expose

# 典型问题以及关键log归纳-todo

```
APM TAG:
APM_AudioPolicyManager
APM_AudioPolicyManagerSPRD

HAL TAG:
audio_hw_primary
```

[**Bug 1385382**](https://bugzilla.unisoc.com/bugzilla/show_bug.cgi?id=1385382) **- [Multimedia][Video][7862][车机]视频播放器不支持MPEG2-TS视频播放** 

```
文件格式（mpeg-ts）支持？但音视频解码（mpeg2+ac3）都不支持：
05-12 17:18:29.152  3819 24467 D MediaCodec: app porc_name:/system/bin/mediaserver mime:video/mpeg2
05-12 17:18:29.152  3819 24467 E NuPlayerDecoder: Failed to create video/mpeg2 decoder
05-12 17:18:29.154  3819 24464 E NuPlayer: received error(0x80000000) from video decoder, flushing(0), now shutting down
05-12 17:18:29.154  3819 24469 D MediaCodec: app porc_name:/system/bin/mediaserver mime:audio/ac3
05-12 17:18:29.154  3819 24469 E NuPlayerDecoder: Failed to create audio/ac3 decoder
05-12 17:18:29.154  3819 24464 E NuPlayer: received error(0x80000000) from audio decoder, flushing(0), now shutting down
```

 **(SPCSS00722052) - [SL8541E_IE_ICT_POS][FIBOCOM][SU808][0]【通话录音】【语音业务中开启通话录音，录音数据异常】**

未正常识别声卡设备

```
M013025  08-24 17:00:47.581   330  3410 E audio_hw:Alsa_Util: open snd control failed.
M013026  08-24 17:00:47.581   330  3410 E audio_hw:Alsa_Util: open snd control failed.
M013027  08-24 17:00:47.581   330  3410 E audio_hw:Alsa_Util: open snd control failed.
M013028  08-24 17:00:47.581   330  3410 E audio_hw:Alsa_Util: There is no one matched to <saudiolte>.
M013029  08-24 17:00:47.582   330  3410 E audio_hw_primary: start_input_stream: voice-call rec cannot open pcm_in driver: cannot open device '/dev/snd/pcmC4294967295D0c': No such file or directory
M01302A  08-24 17:00:47.582   330  3410 E audio_hw_primary: normal rec cannot open pcm_in driver: cannot open device '/dev/snd/pcmC4294967295D0c': No such file or directory
M01302B  08-24 17:00:47.582   330  3410 E audio_hw_primary: start_input_stream  ret error -1
```

[**Bug 1402404**](https://bugzilla.unisoc.com/bugzilla/show_bug.cgi?id=1402404) **(SPCSS00725003) - [SC9820E_IE_ICT_POC][FLYSCALE][FLY480EQ][7276]耳机MIC接的是AUX MIC，如何在耳机插入后打开AUX MIC？**

```
上层使用副MIC将audiosource设置成CAMCORDER就可以
```

# 常用工具

## audio tool连接修改NV

## 音量曲线

```
音量曲线修改手机路劲
vendor/etc/audio_policy_engine_default_stream_volumes.xml
vendor/etc/audio_policy_engine_stream_volumes.xml
 
代码里的路径
device\sprd\对应项目\对应工程\rootdir\system\etc\parameter-framework\audio_policy_engine_stream_volumes.xml
device\sprd\对应项目\对应工程\rootdir\system\etc\parameter-framework\audio_policy_engine_default_stream_volumes.xml
 
device\sprd\对应项目\对应工程\rootdir\system\etc\parameter-framework\audio_policy_engine_default_stream_volumes.xml 
可先在手机验证：将手机文件/vendor/etc/parameter-framework\audio_policy_engine_default_stream_volumes.xml拉出来，修改下面的字段后再push回去并重启手机。
    <reference name="DEFAULT_MEDIA_VOLUME_CURVE">
36     <!-- Default Media reference Volume Curve -->
37         <point>1,-5800</point>
38         <point>20,-4000</point>
39         <point>60,-1700</point>
40         <point>100,0</point>
41     </reference>
======>修改为
    <reference name="DEFAULT_MEDIA_VOLUME_CURVE">
36     <!-- Default Media reference Volume Curve -->
37         <point>1,-6800</point>
38         <point>20,-5000</point>
39         <point>60,-2700</point>
40         <point>100,0</point>
41     </reference>
 
音量曲线调整需要多次反复试验，这组数据如果不满足要求，可以换一组验证, 附件是调试工具
DEFAULT_MEDIA_VOLUME_CURVE这个与耳机设备是同一个参数，调试好后也验下耳机的是否符合要求

androidQ音量曲线对应的是
device/sprd/对应工程/对应项目/rootdir/system/etc/parameter-framework/audio_policy_engine_stream_volumes.xml  

device/sprd/对应工程/对应项目/rootdir/system/etc/parameter-framework/audio_policy_engine_default_stream_volumes.xml  

手机目录
vendor/etc/parameter-framework
```

# Android蓝牙电话(SCO)和蓝牙音乐(A2DP)

# 音频调试

本文介绍了一些与 Android 音频调试有关的提示和技巧。

## Tee Sink

“tee sink”是一种 AudioFlinger 调试功能，仅在定制 build 中提供，用于获取最近音频的短片段以供日后分析。 这方便我们比较实际播放或录制的内容与预期内容。

出于隐私考虑，tee sink 在编译时和运行时均默认处于停用状态。如需使用 tee sink，您需要通过重新编译以及设置属性来启用它。完成调试后，请务必停用此功能；tee sink 在正式 build 中不能保持启用状态。

本部分中的说明适用于 Android 7.x 及更高版本。对于 Android 5.x 和 6.x，请将 `/data/misc/audioserver` 替换为 `/data/misc/media`。此外，您还必须使用 userdebug 或 eng build。如果您使用 userdebug 版本，请使用以下命令停用 verity：

```
adb root && adb disable-verity && adb reboot
```

### 编译时设置

1. `cd frameworks/av/services/audioflinger`
2. 修改 `Configuration.h`。
3. 取消备注 `#define TEE_SINK`。
4. 重新构建 `libaudioflinger.so`。
5. `adb root`
6. `adb remount`
7. 将新的 `libaudioflinger.so` 推送或同步到设备的 `/system/lib`。

### 运行时设置

1. `adb shell getprop | grep ro.debuggable`
   确认输出是：`[ro.debuggable]: [1]`

2. `adb shell`

3. ```
   ls -ld /data/misc/audioserver
   ```

   确认输出是：

   ```
   drwx------ media media ... media
   ```

   如果目录不存在，请按如下方式创建：

   ```
   mkdir /data/misc/audioserver
   chown media:media /data/misc/audioserver
   ```

4. `echo af.tee=# > /data/local.prop`
   其中，`af.tee` 值是一个数字，在下文中有所说明。

5. `chmod 644 /data/local.prop`

6. `reboot`

#### `af.tee` 属性的值

`af.tee` 的值是一个介于 0 到 7 之间的数字，表示几个位的总和（每个功能一个位）。请查看位于 `AudioFlinger.cpp` 中的 `AudioFlinger::AudioFlinger()` 的代码，了解各个位的说明，简单来说就是：

- 1 = 输入
- 2 = FastMixer 输出
- 4 = 各曲目的 AudioRecord 和 AudioTrack

目前还没有针对深度缓冲区和常规混合器的位，不过您可以使用“4”获取类似结果。

### 测试和获取数据

1. 运行您的音频测试。
2. `adb shell dumpsys media.audio_flinger`
3. 在 `dumpsys` 输出中查找如下行：
   `tee copied to /data/misc/audioserver/20131010101147_2.wav`
   这是一个 PCM .wav 文件。
4. 然后，使用 `adb pull` 命令提取任何相关的 `/data/misc/audioserver/*.wav` 文件；请注意，曲目专用的转储文件名不会显示在 `dumpsys` 输出中，但仍会在曲目关闭时保存到 `/data/misc/audioserver`。
5. 在与其他人分享之前，请先查看转储文件是否涉及隐私权问题。

#### 建议

要获取更实用的结果，请尝试以下方法：

- 停用触摸提示音和按键音，以减少测试输出过程中的中断。
- 将所有音量调到最大。
- 停用通过麦克风发出声音或录音的应用（如果这些应用与测试无关）。
- 曲目专用的转储仅在曲目关闭时保存；您可能需要强制关闭某个应用才能转储其曲目专用数据。
- 在测试后立即执行 `dumpsys`；可用的录制空间是有限的。
- 如需确保转储文件不会丢失，请定期将其上传到您的主机。 您只能保留有限数量的转储文件；达到此数量上限后，系统会移除较旧的转储。

### 恢复

如上文所述，tee sink 功能不能保持启用状态。 如需恢复您的版本和设备，请执行以下操作：

1. 还原对 `Configuration.h` 所做的源代码更改。
2. 重新构建 `libaudioflinger.so`。
3. 将恢复后的 `libaudioflinger.so` 推送或同步到设备的 `/system/lib`。
4. `adb shell`
5. `rm /data/local.prop`
6. `rm /data/misc/audioserver/*.wav`
7. `reboot`

## media.log

### ALOGx 宏

Android SDK 中的标准 Java 语言日志记录 API 是 [android.util.Log](http://developer.android.google.cn/reference/android/util/Log.html)。

Android NDK 中的对应 C 语言 API 是 `<android/log.h>` 中声明的 `__android_log_print`。

在 Android 框架的原生部分，我们倾向于使用名为 `ALOGE`、`ALOGW`、`ALOGI`、`ALOGV` 等的宏。这些宏会在 `<utils/Log.h>` 中声明，在本文中，我们将它们统称为 `ALOGx`。

由于所有这些 API 均易于使用和理解，因此在整个 Android 平台中得到了广泛应用。尤其是 `mediaserver` 进程（其中包括 AudioFlinger 声音服务器），它使用了大量 `ALOGx`。

不过，`ALOGx` 和相关项也存在一些限制：

- 它们容易受到“日志垃圾”的影响：日志缓冲区是一种共享资源，因此容易因不相关的日志条目而溢出，进而导致信息缺失。默认情况下，`ALOGV` 变体在编译时处于停用状态。但是，毋庸置疑，如果启用了该变体，就连它也会导致产生日志垃圾。
- 底层内核系统调用会出现阻塞，这可能会导致优先级倒置，因而造成测量干扰和不准确。对于 `FastMixer` 和 `FastCapture` 等对时间要求严格的线程来说，这是需要特别关注的问题。
- 如果出于减少日志垃圾的目的而停用特定日志，则会丢失该日志本应捕获的所有信息。 当明确了特定日志本该是有意义的日志后，已不可能以追溯方式启用它。

### NBLOG、media.log 和 MediaLogService

`NBLOG` API 和关联的 `media.log` 进程以及 `MediaLogService` 服务共同形成较新的媒体日志记录系统，专为解决上述问题而设计。我们将使用术语“media.log”来泛指所有这三个元素。但是严格来说，`NBLOG` 是 C++ 日志记录 API、`media.log`是 Linux 进程名称，而 `MediaLogService` 是用于检查日志的 Android Binder 服务。

`media.log`“时间轴”是一系列相对排序得到保留的日志条目。按照惯例，每个线程都应该使用其自己的时间轴。

### 优势

`media.log` 系统的优势在于：

- 除非需要，否则它不会在主日志中产生垃圾内容。
- 即使在 `mediaserver` 崩溃或挂起时，也可以对其进行检查。
- 在每个时间轴均是非阻塞的。
- 对性能的干扰较小 （当然，没有任何形式的日志记录是完全不会产生干扰的）。

### 架构

以下示意图展示了在引入 `media.log` 之前 `mediaserver` 进程和 `init` 进程之间的关系：

![引入 media.log 之前的架构](https://source.android.google.cn/devices/audio/images/medialog_before.png)

**图 1.** 引入 media.log 之前的架构

下面几点值得注意：

- `init` 可创建 `mediaserver` 分支并执行该进程。
- `init` 可检测到 `mediaserver` 故障，并根据需要重新创建分支。
- 未显示 `ALOGx` 日志记录。

以下示意图展示了将 `media.log` 添加到架构之后组件之间的新关系：

![添加 media.log 之后的架构](https://source.android.google.cn/devices/audio/images/medialog_after.png)

**图 2.** 添加 media.log 之后的架构

重要变更：

- 客户端使用 `NBLOG` API 构建日志条目，并将它们附加到共享内存中的循环缓冲区。
- `MediaLogService` 可以随时转储循环缓冲区的内容。
- 循环缓冲区的设计方式使其具有以下特点：任何共享内存损坏都不会导致 `MediaLogService` 崩溃，该服务仍然能够转储尽可能多的不受损坏影响的缓冲区内容。
- 无论是写入新条目还是读取现有条目，循环缓冲区都是非阻塞和无锁的。
- 无需内核系统调用即可对循环缓冲区（可选时间戳除外）执行写入或读取操作。

#### 使用范围

自 Android 4.4 起，AudioFlinger 中只有几个日志点使用 `media.log` 系统。虽然新 API 不像 `ALOGx` 那样易于使用，但也不是特别难用。我们建议您学习如何使用新的日志记录系统，以便应对必须使用新系统的情况。 特别是，建议您针对必须频繁、定期且无阻塞运行的 AudioFlinger 线程（例如 `FastMixer` 和 `FastCapture` 线程）使用新系统。

### 使用方法

#### 添加日志

首先，您需要将日志添加到代码中。

在 `FastMixer` 和 `FastCapture` 线程中，请使用如下代码：

```
logWriter->log("string");
logWriter->logf("format", parameters);
logWriter->logTimestamp();
```

由于此 `NBLog` 时间轴仅由 `FastMixer` 和 `FastCapture` 线程使用，因此不需要互斥。

在其他 AudioFlinger 线程中，请使用 `mNBLogWriter`：

```
mNBLogWriter->log("string");
mNBLogWriter->logf("format", parameters);
mNBLogWriter->logTimestamp();
```

对于除 `FastMixer` 和 `FastCapture` 之外的线程，线程的 `NBLog` 时间轴既可以由线程本身使用，也可以由 Binder 操作使用。由于 `NBLog::Writer` 不会按时间轴提供任何隐式互斥，因此请确保所有日志都发生在持有线程的互斥 `mLock` 的上下文中。

添加日志后，请重新构建 AudioFlinger。

**注意**：由于时间轴的设计略去了互斥，因此每个线程需要有单独的 `NBLog::Writer` 时间轴，以确保线程安全性。如果您希望多个线程使用同一时间轴，则可以使用现有的互斥进行保护（如上文中有关 `mLock` 的部分所述）。或者，您也可以使用 `NBLog::LockedWriter` 封装容器而非 `NBLog::Writer`。不过，这会使该 API 的以下这项主要优势无效：非阻塞行为。

完整的 `NBLog` API 位于 `frameworks/av/include/media/nbaio/NBLog.h`。

#### 启用 media.log

默认情况下，`media.log` 处于停用状态。仅当 `ro.test_harness` 属性设为 `1` 时，它才处于活动状态。启用方式如下：

```
adb root
adb shell
echo ro.test_harness=1 > /data/local.prop
chmod 644 /data/local.prop
reboot
```

重新启动时会丢失连接，因此：

```
adb shell
```

`ps media` 命令现在将显示两个进程：

- media.log
- mediaserver

请记下 `mediaserver` 的进程 ID，以供稍后使用。

![image-20200903143842554](C:\Users\hursion.zhang\AppData\Roaming\Typora\typora-user-images\image-20200903143842554.png)

#### 显示时间轴

您可以随时手动请求日志转储。 以下命令会显示来自所有处于活动状态的近期时间轴的日志，然后将其清除：

```
dumpsys media.log
```

请注意，根据设计，时间轴是独立的，没有用于合并时间轴的功能。

#### 在出现 mediaserver 故障后恢复日志

现在尝试终止 `mediaserver` 进程：`kill -9 #`，其中 # 是您之前记下的进程 ID。您应该会在主 `logcat` 中看到 `media.log` 的转储，其中显示了发生崩溃之前的所有日志。

```
dumpsys media.log
```

# 音频术语

与音频相关的术语，其中包括广泛使用的通用术语和 Android 专用术语。

## 通用术语

与音频相关的通用术语的含义都采用其约定俗成的解释。

### 数字音频

数字音频术语涉及使用以数字格式编码的音频信号处理声音。如需了解详情，请参阅[数字音频](http://en.wikipedia.org/wiki/Digital_audio)。

* **acoustics**/声学

* **attenuation**/衰减

* **audiophile**/音响发烧友

* **bits per sample or bit depth**/每样本位数或位深

  每个样本的信息位数。

* **channel**/声道

  单个音频信息流，通常与一个录音位置或播放位置相对应。

* **downmixing**/缩混

* **duck**/闪避

* **frame**/帧

  某个时间点上的样本集，每个声道对应一个样本。

* **frames per buffer**/每缓冲区帧数

  同时从一个模块传递到另一个模块的帧数。音频 HAL 接口会使用每缓冲区帧数这一概念。

* **gain**/增益

* **Hz**

  采样率或帧率的单位。

* **high-resolution audio**/高解析度音频

* **latency**/延时

  信号通过系统时的延迟时间。

* **lossless**/无损

* **lossy**/有损

* **mono**/单声道

* **multichannel**/多声道

* **mute**/静音

  暂时强制将音量降为 0；独立于通常使用的音量控件。

* **overrun**/溢出

* **panning**/平移

* **PCM**

  脉冲编码调制。最常见的低级别数字音频编码形式。以有规律的间隔对音频信号取样（称为采样率），然后根据位深对特定范围内的离散值进行量化。例如，对于 16 位 PCM，样本值是介于 -32768 到 +32767 之间的整数值。

* **ramp**/斜坡

* **sample**/样本

  代表某个时间点上一个声道的音频值的数字。

* **sample rate or frame rate**/采样率或帧率

  每秒帧数。“帧率”这一用法更为准确，但业内习惯使用“采样率”来表示帧率。

* **sonification**/可听化

* **stereo**/立体声

* **stereo widening**/立体声展宽。

* **surround sound**/环绕声

* **transparency**/透明度

* **underrun**/欠载

* **upmixing**/扩混

* **virtualizer**/虚拟音效

* **volume**/音量

### 设备间互连

设备间互连技术用于将各设备的音频和视频组件连接起来，用户可通过外部连接器从视觉上直观感受到这些技术。HAL 实现人员和最终用户应了解以下术语。

- **Bluetooth**/蓝牙

  近距离无线技术。要详细了解与音频相关的[蓝牙规范](http://en.wikipedia.org/wiki/Bluetooth_profile)和[蓝牙协议](http://en.wikipedia.org/wiki/Bluetooth_protocols)，请参阅 [A2DP](http://en.wikipedia.org/wiki/Bluetooth_profile#Advanced_Audio_Distribution_Profile_.28A2DP.29) 来了解音乐方面的信息，参阅 [SCO](http://en.wikipedia.org/wiki/Bluetooth_protocols#Synchronous_connection-oriented_.28SCO.29_link) 来了解电话方面的信息；另外还可以参阅[音频/视频远程控制规范 (AVRCP)](http://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Audio.2FVideo_Remote_Control_Profile_.28AVRCP.29)。

- **DisplayPort**

  视频电子标准协会 (VESA) 制订的数字显示接口。

- **dongle**/外接小配件

  [外接小配件](https://en.wikipedia.org/wiki/Dongle)是一种小工具，特指可直接插入到其他设备上的小工具。

- **FireWire**

  参阅 IEEE 1394。

- **HDMI**

  高清晰度多媒体接口，用于传输音频和视频数据。移动设备上会使用微型 HDMI（D 型）或 MHL 连接器。

- **IEEE 1394**

  [IEEE 1394](https://en.wikipedia.org/wiki/IEEE_1394)（也称为 FireWire）是一种用于实时低延迟应用（如音频）的串行总线。

- **Intel HDA**

  Intel 高清晰度音频（请不要与泛指的“高清晰度音频”或“高解析度音频”混淆）；一种前面板连接器规范。如需了解详情，请参阅 [Intel 高清晰度音频](http://en.wikipedia.org/wiki/Intel_High_Definition_Audio)。

- **interface**/接口

  [接口](https://en.wikipedia.org/wiki/Interface_(computing))可将信号从一种表现形式转换为另一种。常见的接口包括 USB 音频接口和 MIDI 接口。

- **line level**/线路电平

  [线路电平](http://en.wikipedia.org/wiki/Line_level)是指在不同音频组件（而非换能器）之间传输的模拟音频信号的强度。

- **MHL**

  移动高清连接技术。一种移动音频/视频接口，通常通过 micro-USB 连接器进行数据传输。

- **phone connector**/手机连接器

  连接设备和有线头戴式耳机、耳麦或线路电平放大器的小型或超小型组件。

- **SlimPort**

  micro-USB 转 HDMI 的适配器。

- **S/PDIF**

  Sony/Philips 数字接口格式，用于未压缩的 PCM 音频的互连。如需了解详情，请参阅 [S/PDIF](http://en.wikipedia.org/wiki/S/PDIF)。S/PDIF 是 [AES3](https://en.wikipedia.org/wiki/AES3) 的消费级版本。

- **Thunderbolt**

  与 USB 和 HDMI 竞争的一种多媒体接口，用于连接高端外围设备。如需了解详情，请参阅 [Thunderbolt](http://en.wikipedia.org/wiki/Thunderbolt_(interface))。

- **TOSLINK**

  [TOSLINK](https://en.wikipedia.org/wiki/TOSLINK) 是一种配合 S/PDIF 使用的光纤音频数据线。

- **USB**

  通用串行总线。如需了解详情，请参阅 [USB](http://en.wikipedia.org/wiki/USB)。

### 设备内互连

设备内互连技术用于将一台设备内的不同音频组件连接起来，如果不拆开设备，则无法从视觉上直观感受到这些技术。HAL 实现人员可能需要了解以下术语，但最终用户不需要。要详细了解设备内互连，请参阅下列文章：

- [GPIO](http://en.wikipedia.org/wiki/General-purpose_input/output)
- [I²C](http://en.wikipedia.org/wiki/I²C)，用于控制声道
- [I²S](http://en.wikipedia.org/wiki/I²S)，用于音频数据，比 SLIMbus 简单
- [McASP](http://en.wikipedia.org/wiki/McASP)
- [SLIMbus](http://en.wikipedia.org/wiki/SLIMbus)
- [SPI](http://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus)
- [AC'97](http://en.wikipedia.org/wiki/AC'97)
- [Intel HDA](http://en.wikipedia.org/wiki/Intel_High_Definition_Audio)
- [SoundWire](http://mipi.org/specifications/soundwire)

在 [ALSA 系统芯片 (ASoC)](http://www.alsa-project.org/main/index.php/ASoC) 中，以上各项统称为[数字音频接口](https://www.kernel.org/doc/Documentation/sound/soc/dai.rst) (DAI)。

### 音频信号路径

音频信号路径术语涉及音频数据从应用传输到换能器（反之亦然）的信号路径。

- **ADC**

  模拟转数字转换器。用于将模拟信号（在时间和振幅上保持连续的信号）转换为数字信号（在时间和振幅上离散的信号）的模块。从概念上讲，一个 ADC 包含一个周期性采样保持器，后跟一个量化器（尽管并不一定需要采用这种方式）。ADC 前面通常有一个低通滤波器，用来滤除通过目标采样率无法呈现的所有高频分量。如需了解详情，请参阅[模拟转数字转换器](http://en.wikipedia.org/wiki/Analog-to-digital_converter)。

- **AP**

  应用处理器。移动设备上的主要通用计算系统。

- **codec**/编解码器

  编码器和解码器，用于将音频信号从一种表现形式编码和/或解码成另一种表现形式（通常是从模拟信号到 PCM 或从 PCM 到模拟信号）。严格来讲，“编解码器”同时指编码和解码模块，但也可仅泛指其中一个。如需了解详情，请参阅[音频编解码器](http://en.wikipedia.org/wiki/Audio_codec)。

- **DAC**

  数字转模拟转换器，用于将数字信号（在时间和振幅上离散的信号）转换为模拟信号（在时间和振幅上保持连续的信号）的模块。DAC 后面通常有一个低通滤波器，用来滤除由数字量化引入的高频分量。如需了解详情，请参阅[数字转模拟转换器](http://en.wikipedia.org/wiki/Digital-to-analog_converter)。

- **DSP**

  数字信号处理器。可选组件，通常位于应用处理器之后（用于输出）或之前（用于输入）。主要用途是减轻应用处理器的负担，并以较低的功耗提供信号处理功能。

- **PDM**

  脉冲密度调制。用于按数字信号表示模拟信号的调制形式，其中相对密度 1s 和 0s 表示信号电平。通常用于数字转模拟转换器。如需了解详情，请参阅[脉冲密度调制](http://en.wikipedia.org/wiki/Pulse-density_modulation)。

- **PWM**

  脉冲宽度调制。用于按数字信号表示模拟信号的调制形式，其中数字脉冲的相对宽度表示信号电平。通常用于模拟转数字转换器。如需了解详情，请参阅[脉冲宽度调制](http://en.wikipedia.org/wiki/Pulse-width_modulation)。

- **transducer**/换能器

  将现实世界物理量中的变量转换为电信号。在音频中，物理量是声压，而换能器是扬声器和麦克风。如需了解详情，请参阅[换能器](http://en.wikipedia.org/wiki/Transducer)。

### 采样率转换

采样率转换术语涉及从一种采样率转换为另一种采样率的过程。

- **downsample**/降采样

  重新采样，其中接收器采样率 < 信号源采样率。

- **Nyquist frequency**/奈奎斯特频率

  可由离散信号以指定采样率的一半表示的最大频率分量。例如，由于人类的听力范围可达到近 20 kHz，因此数字音频信号的采样率必须至少有 40 kHz 才能代表该范围。在实践中，44.1 kHz 和 48 kHz 的采样率比较常用，对应的奈奎斯特频率分别为 22.05 kHz 和 24 kHz。如需了解详情，请参阅[奈奎斯特频率](http://en.wikipedia.org/wiki/Nyquist_frequency)和[听力范围](http://en.wikipedia.org/wiki/Hearing_range)。

- **resampler**/重采样器

  采样率转换器的同义词。

- **resampling**/重新采样

  转换采样率的过程。

- **sample rate converter**/采样率转换器

  执行重新采样的模块。

- **sink**/接收器

  重采样器的输出端。

- **source**/信源

  重采样器的输入端。

- **upsample**/升采样

  重新采样，其中接收器采样率 > 信源采样率。

## Android 专用术语

Android 专用术语包括仅在 Android 音频框架中使用的术语，以及在 Android 中具有特殊意义的通用术语。

- **ALSA**

  高级 Linux 声音体系。Linux 的音频框架，对其他系统也有影响。要了解通用定义，请参阅 [ALSA](http://en.wikipedia.org/wiki/Advanced_Linux_Sound_Architecture)。在 Android 中，ALSA 指的是内核音频框架和驱动程序，而不是用户模式 API。另请参阅“tinyalsa"。

- **audio device**/音频设备

  以 HAL 实现为基础的音频 I/O 端点。

- **AudioEffect**

  用于输出（处理后）音效与输入（处理前）音效的 API 和实现框架。该 API 在 [android.media.audiofx.AudioEffect](http://developer.android.google.cn/reference/android/media/audiofx/AudioEffect.html?hl=zh-cn) 中定义。

- **AudioFlinger**

  Android 声音服务器实现用例。AudioFlinger 在 mediaserver 进程中运行。要了解通用定义，请参阅[声音服务器](http://en.wikipedia.org/wiki/Sound_server)。

- **audio focus**/音频焦点

  跨多个独立应用管理音频互动的 API 集。如需了解详情，请参阅[管理音频焦点](http://developer.android.google.cn/training/managing-audio/audio-focus.html?hl=zh-cn)以及 [android.media.AudioManager](http://developer.android.google.cn/reference/android/media/AudioManager.html?hl=zh-cn) 的与焦点相关的方法和常量。

- **AudioMixer**

  AudioFlinger 中的模块，负责合并多个音轨以及应用衰减（音量）和音效。要了解通用定义，请参阅[混音（录制的音乐）](http://en.wikipedia.org/wiki/Audio_mixing_(recorded_music))（将混合器当做一个硬件设备或软件应用而非系统中的软件模块）。

- **audio policy**/音频政策

  负责所有需要先做出政策决策的操作的服务，例如打开新的 I/O 音频流、更改后重新路由，以及音频流音量管理。

- **AudioRecord**

  用于从麦克风等音频输入设备接收数据的主要低级别客户端 API。相应数据通常为 PCM 格式。该 API 在 [android.media.AudioRecord](http://developer.android.google.cn/reference/android/media/AudioRecord.html?hl=zh-cn) 中定义。

- **AudioResampler**

  AudioFlinger 中的模块，负责[采样率转换](https://source.android.google.cn/devices/audio/src?hl=zh-cn)。

- **audio source**/音频来源

  一个常量枚举，用于为捕获音频输入指明目标使用情形。如需了解详情，请参阅[音频来源](http://developer.android.google.cn/reference/android/media/MediaRecorder.AudioSource.html?hl=zh-cn)。对于 21 级及以上级别的 API，建议使用[音频属性](https://source.android.google.cn/devices/audio/attributes?hl=zh-cn)。

- **AudioTrack**

  用于向音响设备等音频输出设备发送数据的主要低级别客户端 API。相应数据通常为 PCM 格式。该 API 在 [android.media.AudioTrack](http://developer.android.google.cn/reference/android/media/AudioTrack.html?hl=zh-cn) 中定义。

- **audio_utils**

  提供 PCM 格式转换、WAV 文件 I/O 以及[非阻塞 FIFO](https://source.android.google.cn/devices/audio/avoiding_pi?hl=zh-cn#nonBlockingAlgorithms) 等功能的音频实用程序库，很大程度上独立于 Android 平台。

- **client**/客户端

  通常指一个应用或应用客户端。不过，AudioFlinger 客户端可以是一个在 mediaserver 系统进程中运行的线程，例如，播放由 MediaPlayer 对象解码的媒体内容时。

- **HAL**

  硬件抽象层。HAL 在 Android 中是通用术语；涉及音频时，它是介于 AudioFlinger 和内核设备驱动程序之间的一个层，具有一个 C 语言编写的 API（取代了 C++ libaudio）。

- **FastCapture**

  AudioFlinger 中的线程。经配置，它会向延迟时间较短的 fast track 发送音频数据，并驱动输入设备的运行，以缩短延迟时间。

- **FastMixer**

  AudioFlinger 中的线程。经配置，它会从延迟时间较短的 fast track 接收并混合音频数据，同时驱动主要输出设备的运行，以缩短延迟时间。

- **fast track**

  部分设备和路由中具有较短延迟时间但功能较少的 AudioTrack 或 AudioRecord 客户端。

- **MediaPlayer**

  比 AudioTrack 级别更高的客户端 API，播放已编码的内容或包含多媒体音频和视频轨道的内容。

- **media.log**

  仅在定制版本中提供的 AudioFlinger 调试功能。用于在环形缓冲区中记录音频事件，然后可以根据需要倒回去撤消这些活动。

- **mediaserver**

  Android 系统进程，包含 AudioFlinger 等与媒体相关的服务。

- **NBAIO**

  非阻塞音频输入/输出，AudioFlinger 端口的抽象表示。此术语有一定的歧义，因为部分 NBAIO API 实现用例支持阻塞。NBAIO 的主要实现用例适用于不同的管道类型。

- **normal mixer**/常规混合器

  AudioFlinger 中的线程，可用于大部分功能完善的 AudioTrack 客户端。它能直接驱动输出设备的运行，或通过管道将其子混音输入 FastMixer。

- **OpenSL ES**

  [Khronos 集团](http://www.khronos.org/)推行的音频 API 标准。API 级别为 9 或更高级别的 Android 版本都支持原生音频 API（基于 [OpenSL ES 1.0.1](http://www.khronos.org/opensles/) 的部分标准）。

- **silent mode**/静音模式

  可由用户设置、用于将手机振铃器和通知设为静音而不会影响媒体内容播放（音乐、视频和游戏）或闹钟的功能。

- **SoundPool**

  比 AudioTrack 级别更高的客户端 API，可播放通过采样得到的音频片段，适用于触发界面反馈、游戏音效等。该 API 在 [android.media.SoundPool](http://developer.android.google.cn/reference/android/media/SoundPool.html?hl=zh-cn) 中定义。

- **Stagefright**

  请参阅[媒体](https://source.android.google.cn/devices/media?hl=zh-cn)。

- **StateQueue**

  AudioFlinger 中的模块，负责同步线程之间的状态。NBAIO 用于传递数据，而 StateQueue 用于传递控制信息。

- **strategy**/策略

  具有类似行为的音频流类型构成的组，用于音频政策服务。

- **stream type**/音频流类型

  表示音频输出使用情形的枚举。音频政策的实现用例会根据音频流类型以及其他参数来确定音量和路由决策。要查看音频流类型的列表，请参阅 [android.media.AudioManager](http://developer.android.google.cn/reference/android/media/AudioManager.html?hl=zh-cn)。

- **tee sink**

  请参阅[音频调试](https://source.android.google.cn/devices/audio/debugging?hl=zh-cn#teeSink)。

- **tinyalsa**

  ALSA 内核之上采用 BSD 许可的小型用户模式 API。建议用于实现 HAL。

- **ToneGenerator**

  比 AudioTrack 级别更高的客户端 API，用于播放双音多频 (DTMF) 信号。如需了解详情，请参阅[双音多频信号](http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling)和 [android.media.ToneGenerator](http://developer.android.google.cn/reference/android/media/ToneGenerator.html?hl=zh-cn) 处的 API 定义。

- **track**/音轨

  音频流。由 AudioTrack 或 AudioRecord API 控制。

- **volume attenuation curve**/音量衰减曲线

  对于给定输出，音量衰减曲线指从通用音量指数到特定衰减因数的设备专属映射。

- **volume index**/音量指数

  表示某个音频流的目标相对音量的整数（没有单位）。[android.media.AudioManager](http://developer.android.google.cn/reference/android/media/AudioManager.html?hl=zh-cn) 的音量相关 API 在运行时采用音量指数（而非绝对的衰减因数）。

# 任务表：

1. 在 Android 平台绘制一张图片，使用至少 3 种不同的 API，ImageView，SurfaceView，自定义 View
2. 在 Android 平台使用 AudioRecord 和 MediaPlayer API 完成音频 PCM 数据的采集和播放，并实现读写音频 wav 文件
3. 在 Android 平台使用 Camera API 进行视频的采集，分别使用 SurfaceView、TextureView 来预览 Camera 数据，取到 NV21 的数据回调
4. 学习 Android 平台的 MediaExtractor 和 MediaMuxer API，知道如何解析和封装 mp4 文件
5. 学习 Android 平台 OpenGL ES API，了解 OpenGL 开发的基本流程，使用 OpenGL 绘制一个三角形
6. 学习 Android 平台 OpenGL ES API，学习纹理绘制，能够使用 OpenGL 显示一张图片
7. 学习 MediaCodec API，完成音频 AAC 硬编、硬解
8. 学习 MediaCodec API，完成视频 H.264 的硬编、硬解
9. 串联整个音视频录制流程，完成音视频的采集、编码、封包成 mp4 输出
10. 串联整个音视频播放流程，完成 mp4 的解析、音视频的解码、播放和渲染
11. 进一步学习 OpenGL，了解如何实现视频的剪裁、旋转、水印、滤镜，并学习 OpenGL 高级特性，如：VBO，VAO，FBO 等等
12. 学习 Android 图形图像架构，能够使用 GLSurfaceviw 绘制 Camera 预览画面
13. 深入研究音视频相关的网络协议，如 rtmp，hls，以及封包格式，如：flv，mp4
14. 深入学习一些音视频领域的开源项目，如 webrtc，ffmpeg，ijkplayer，librtmp 等等
15. 将 ffmpeg 库移植到 Android 平台，结合上面积累的经验，编写一款简易的音视频播放器
16. 将 x264 库移植到 Android 平台，结合上面积累的经验，完成视频数据 H264 软编功能
17. 将 librtmp 库移植到 Android 平台，结合上面积累的经验，完成 Android RTMP 推流功能
18. 上面积累的经验，做一款短视频 APP，完成如：断点拍摄、添加水印、本地转码、视频剪辑、视频拼接、MV 特效等功能

问题：

Q1.如何自定义native service？